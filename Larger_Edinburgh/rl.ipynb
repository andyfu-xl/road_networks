{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "# sumoBinary = checkBinary('sumo-gui')\n",
    "sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervehicleConnectivity(threshold = None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for vehicle in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(vehicle)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).to(device).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).to(device).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        intervehicle_distances = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    return intervehicle_distances, xs.to(\"cpu\").squeeze(), ys.to(\"cpu\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrips(dur=1000, density=12):\n",
    "    os.system(\"python $SUMO_HOME/tools/randomTrips.py -n config/osm.net.xml.gz -r config/osm.passenger.trips.xml -e \" + str(dur) + \" -l --insertion-density=\" + str(density))\n",
    "\n",
    "def shouldContinueSim():\n",
    "    numVehicles = traci.simulation.getMinExpectedNumber()\n",
    "    return True if numVehicles > 0 else False\n",
    "\n",
    "def restart(sumoCmd):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "def close():\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Knowledges:\n",
    "    def __init__(self):\n",
    "        self.knowledges = {}\n",
    "        self.delays = {}\n",
    "    \n",
    "    def add_observations(self, vehicles, observed_vehicles):\n",
    "        for vehicle, visibility in zip(vehicles, observed_vehicles):\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = []\n",
    "                self.delays[vehicle] = 0\n",
    "            self.knowledges[vehicle].append(int(visibility))\n",
    "            if visibility == 0:\n",
    "                self.delays[vehicle] += 1\n",
    "            else:\n",
    "                self.delays[vehicle] = 0\n",
    "    \n",
    "    def merge_knowledges(self, new_knowledges, new_delays):\n",
    "        prev_missing, prev_delay = self.evaluate_knowledge()\n",
    "        for vehicle, visibility in new_knowledges.items():\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = copy.deepcopy(visibility)\n",
    "                self.delays[vehicle] = new_delays[vehicle]\n",
    "            else:\n",
    "                for i in range(1, len(self.knowledges[vehicle])+1):\n",
    "                    if i > len(visibility):\n",
    "                        break\n",
    "                    self.knowledges[vehicle][-i] = visibility[-i] | self.knowledges[vehicle][-i]\n",
    "                self.delays[vehicle] = min(self.delays[vehicle], new_delays[vehicle])\n",
    "        new_missing, new_delay = self.evaluate_knowledge()\n",
    "        return copy.deepcopy(self.knowledges), copy.deepcopy(self.delays), prev_missing - new_missing, prev_delay - new_delay\n",
    "\n",
    "    def get_knowledges(self):\n",
    "        return copy.deepcopy(self.knowledges)\n",
    "    \n",
    "    def get_delays(self):\n",
    "        return copy.deepcopy(self.delays)\n",
    "    \n",
    "    def evaluate_knowledge(self):\n",
    "        observed = 0\n",
    "        delay = 0\n",
    "        num_vehicles = len(self.knowledges)\n",
    "        for vehicle, visibility in self.knowledges.items():\n",
    "            observed += sum(visibility)\n",
    "            delay += self.delays[vehicle]\n",
    "        return 1-(observed / num_vehicles), delay / num_vehicles\n",
    "\n",
    "class Beacon:\n",
    "    def __init__(self, trace_hidden):\n",
    "        self.trace_hidden = copy.deepcopy(trace_hidden)\n",
    "    \n",
    "    def update(self, trace_hidden):\n",
    "        self.trace_hidden = copy.deepcopy(trace_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'reward'))\n",
    "\n",
    "class GRU_RL(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRU_RL, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n",
      "Simulation ended at time: 14.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 211.44s\n",
      " TraCI-Duration: 0.01s\n",
      " Real time factor: 0.0662136\n",
      " UPS: 0.089861\n",
      "Vehicles: \n",
      " Inserted: 2 (Loaded: 25)\n",
      " Running: 2\n",
      " Waiting: 0\n",
      "Statistics (avg of 0):\n",
      " RouteLength: 0.00\n",
      " Speed: 0.00\n",
      " Duration: 0.00\n",
      " WaitingTime: 0.00\n",
      " TimeLoss: 0.00\n",
      " DepartDelay: 0.00\n",
      "\n",
      "***Starting server on port 45355 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (104ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "Simulation ended at time: 1100.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 1.78s\n",
      " TraCI-Duration: 1.56s\n",
      " Real time factor: 616.938\n",
      " UPS: 25339.315760\n",
      "Vehicles: \n",
      " Inserted: 111\n",
      " Running: 52\n",
      " Waiting: 0\n",
      "Statistics (avg of 59):\n",
      " RouteLength: 4182.20\n",
      " Speed: 9.57\n",
      " Duration: 445.41\n",
      " WaitingTime: 15.56\n",
      " TimeLoss: 61.44\n",
      " DepartDelay: 0.48\n",
      "\n",
      "Max action space:  15\n"
     ]
    }
   ],
   "source": [
    "# find the maximum action space\n",
    "randomTrips(1000, 1.5)\n",
    "restart(sumoCmd)\n",
    "max_action_space = 0\n",
    "total_actions = 0\n",
    "\n",
    "total_missing_gain = 0\n",
    "total_delay_gain = 0\n",
    "step = 0\n",
    "max_action_space = 0\n",
    "\n",
    "\n",
    "while shouldContinueSim():\n",
    "    step += 1\n",
    "    if step > 1100:\n",
    "        close()\n",
    "        break\n",
    "    traci.simulationStep()\n",
    "    ids = traci.vehicle.getIDList()\n",
    "    connectivity, xs, ys = intervehicleConnectivity(800)\n",
    "    # minus the diagonal\n",
    "    action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "    for i, vehicle in enumerate(ids):\n",
    "        # get non-zero indices except the diagonal\n",
    "        non_zero_indices = np.where(action_spaces[i] == 1)[0]\n",
    "        if len(non_zero_indices) > max_action_space:\n",
    "            max_action_space = len(non_zero_indices)\n",
    "print(\"Max action space: \", max_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumoGym(gym.Env):\n",
    "    def __init__(self, sumoCmd, max_action_space, max_steps=1100):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.max_action_space = max_action_space\n",
    "        self.max_steps = max_steps\n",
    "        self.step_counter = 0\n",
    "        self.vehicle_knowledges = {}\n",
    "        self.vehicle_ids = None\n",
    "        self.action_spaces = None\n",
    "        self.xs = None\n",
    "        self.ys = None\n",
    "\n",
    "    def act(self, vehicle, selected_index):\n",
    "        print(\"Vehicle: \", vehicle, \" selected index: \", selected_index)\n",
    "        selected_index = random.choice(non_zero_indices)\n",
    "        receiver = self.vehicle_ids[selected_index]\n",
    "        _, _, missing_gain, delay_gain = self.vehicle_knowledges[receiver].merge_knowledges(self.vehicle_knowledges[vehicle].get_knowledges(), self.vehicle_knowledges[vehicle].get_delays())\n",
    "        reward = self.getRewards(missing_gain, delay_gain)\n",
    "        return reward\n",
    "    \n",
    "    def step(self):\n",
    "        if self.getDoneState():\n",
    "            print(\"Simulation is done.\")\n",
    "            traci.close()\n",
    "            return True\n",
    "        else:\n",
    "            traci.simulationStep()\n",
    "            self.step_counter += 1\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            connectivity, self.xs, self.ys = intervehicleConnectivity(800)\n",
    "            # minus the diagonal\n",
    "            self.action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "            \n",
    "            for i, vehicle in enumerate(self.vehicle_ids):\n",
    "                if vehicle not in self.vehicle_knowledges:\n",
    "                    self.vehicle_knowledges[vehicle] = Knowledges()\n",
    "                self.vehicle_knowledges[vehicle].add_observations(self.vehicle_ids, connectivity[i])\n",
    "            return False\n",
    "        \n",
    "    def render(self):\n",
    "        self.show_gui = True\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "        while not self.getDoneState():\n",
    "            self.step()\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            if len(self.vehicle_ids)>1:\n",
    "                break\n",
    "\n",
    "        return self.getCurrentStates()\n",
    "\n",
    "    def getCurrentStates(self):\n",
    "        \"\"\"\n",
    "        function: Get all the states of vehicles, observation space.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for i in range(len(self.action_spaces)):\n",
    "            # pad both xs and ys to the max action space\n",
    "            other_vehicles_xs = (self.xs * self.action_spaces[i])\n",
    "            other_vehicles_ys = (self.ys * self.action_spaces[i])\n",
    "            other_vehicles_xs = F.pad(other_vehicles_xs, (0, self.max_action_space - other_vehicles_xs.size(0)), \"constant\", 0)\n",
    "            other_vehicles_ys = F.pad(other_vehicles_ys, (0, self.max_action_space - other_vehicles_ys.size(0)), \"constant\", 0)\n",
    "            states.append(torch.cat((other_vehicles_xs, other_vehicles_ys)).view(-1))\n",
    "        states = torch.stack(states)\n",
    "        return states, self.vehicle_ids\n",
    "\n",
    "        \n",
    "    def getRewards(self, missing_reduction, delay_reduction, missing_coefficient=2, delay_coefficient=3):\n",
    "        return missing_reduction * missing_coefficient + delay_reduction * delay_coefficient\n",
    "        \n",
    "    def getDoneState(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter <= self.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended at time: 12.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 5.64s\n",
      " TraCI-Duration: 0.01s\n",
      " Real time factor: 2.12615\n",
      " UPS: 2.657690\n",
      "Vehicles: \n",
      " Inserted: 2 (Loaded: 25)\n",
      " Running: 2\n",
      " Waiting: 0\n",
      "Statistics (avg of 0):\n",
      " RouteLength: 0.00\n",
      " Speed: 0.00\n",
      " Duration: 0.00\n",
      " WaitingTime: 0.00\n",
      " TimeLoss: 0.00\n",
      " DepartDelay: 0.00\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 51359 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (109ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = max_action_space\n",
    "# Get the number of state observations\n",
    "env = SumoGym(sumoCmd, max_action_space)\n",
    "state, v_ids = env.reset()\n",
    "n_observations = max_action_space * 2\n",
    "\n",
    "policy_net = GRU_RL(input_size = n_observations, hidden_size=128, output_size=n_actions, num_layers=5).to(device)\n",
    "target_net = GRU_RL(input_size = n_observations, hidden_size=128, output_size=n_actions, num_layers=5).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, hidden=None):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    with torch.no_grad():\n",
    "        out, out_hidden = policy_net(state.unsqueeze(0), hidden)\n",
    "        if sample > eps_threshold:\n",
    "                return out.max(1).indices.view(1, 1), out_hidden\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long), out_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended at time: 21.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 19.19s\n",
      " TraCI-Duration: 0.04s\n",
      " Real time factor: 1.09455\n",
      " UPS: 1.876368\n",
      "Vehicles: \n",
      " Inserted: 3 (Loaded: 25)\n",
      " Running: 3\n",
      " Waiting: 0\n",
      "Statistics (avg of 0):\n",
      " RouteLength: 0.00\n",
      " Speed: 0.00\n",
      " Duration: 0.00\n",
      " WaitingTime: 0.00\n",
      " TimeLoss: 0.00\n",
      " DepartDelay: 0.00\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 39463 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (105ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Transition' object has no attribute 'next_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m memory\u001b[38;5;241m.\u001b[39mpush(state, action, reward)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     33\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[214], line 10\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m Transition(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute a mask of non-final states and concatenate the batch elements\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# (a final state would've been the one after which simulation ended)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 10\u001b[0m                                       \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_state\u001b[49m)), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     11\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mnext_state\n\u001b[1;32m     12\u001b[0m                                             \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     13\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transition' object has no attribute 'next_state'"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    states, v_ids = env.reset()\n",
    "    states = states.to(device)\n",
    "    done = False\n",
    "    hiddens = {}\n",
    "    while not done:\n",
    "        done = env.step()\n",
    "        states, v_ids = env.getCurrentStates()\n",
    "        for i in range(states.size(0)):\n",
    "            curr_v = v_ids[i]\n",
    "            state = states[i].to(device)\n",
    "            if curr_v not in hiddens:\n",
    "                action, v_hidden = select_action(state)\n",
    "            else:\n",
    "                action, v_hidden = select_action(state, hiddens[curr_v])\n",
    "            hiddens[curr_v] = v_hidden\n",
    "            if action >= len(env.action_spaces[i]) or env.action_spaces[i][action.item()] == 0:\n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else:\n",
    "                reward = env.act(v_ids[i], action)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "            memory.push(state, action, reward)\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            optimize_model()\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
