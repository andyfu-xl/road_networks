{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "# sumoBinary = checkBinary('sumo-gui')\n",
    "sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervehicleConnectivity(threshold = None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for vehicle in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(vehicle)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).to(device).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).to(device).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        intervehicle_distances = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    return intervehicle_distances, xs.to(\"cpu\").squeeze(), ys.to(\"cpu\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrips(dur=1000, density=12):\n",
    "    os.system(\"python $SUMO_HOME/tools/randomTrips.py -n config/osm.net.xml.gz -r config/osm.passenger.trips.xml -e \" + str(dur) + \" -l --insertion-density=\" + str(density))\n",
    "\n",
    "def shouldContinueSim():\n",
    "    numVehicles = traci.simulation.getMinExpectedNumber()\n",
    "    return True if numVehicles > 0 else False\n",
    "\n",
    "def restart(sumoCmd):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "def close():\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Knowledges:\n",
    "    def __init__(self):\n",
    "        self.knowledges = {}\n",
    "        self.delays = {}\n",
    "    \n",
    "    def add_observations(self, vehicles, observed_vehicles):\n",
    "        for vehicle, visibility in zip(vehicles, observed_vehicles):\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = []\n",
    "                self.delays[vehicle] = 0\n",
    "            self.knowledges[vehicle].append(int(visibility))\n",
    "            if visibility == 0:\n",
    "                self.delays[vehicle] += 1\n",
    "            else:\n",
    "                self.delays[vehicle] = 0\n",
    "    \n",
    "    def merge_knowledges(self, new_knowledges, new_delays):\n",
    "        prev_missing, prev_delay = self.evaluate_knowledge()\n",
    "        for vehicle, visibility in new_knowledges.items():\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = copy.deepcopy(visibility)\n",
    "                self.delays[vehicle] = new_delays[vehicle]\n",
    "            else:\n",
    "                for i in range(1, len(self.knowledges[vehicle])+1):\n",
    "                    if i > len(visibility):\n",
    "                        break\n",
    "                    self.knowledges[vehicle][-i] = visibility[-i] | self.knowledges[vehicle][-i]\n",
    "                self.delays[vehicle] = min(self.delays[vehicle], new_delays[vehicle])\n",
    "        new_missing, new_delay = self.evaluate_knowledge()\n",
    "        return copy.deepcopy(self.knowledges), copy.deepcopy(self.delays), prev_missing - new_missing, prev_delay - new_delay\n",
    "\n",
    "    def get_knowledges(self):\n",
    "        return copy.deepcopy(self.knowledges)\n",
    "    \n",
    "    def get_delays(self):\n",
    "        return copy.deepcopy(self.delays)\n",
    "    \n",
    "    def evaluate_knowledge(self):\n",
    "        observed = 0\n",
    "        delay = 0\n",
    "        num_vehicles = len(self.knowledges)\n",
    "        for vehicle, visibility in self.knowledges.items():\n",
    "            observed += sum(visibility)\n",
    "            delay += self.delays[vehicle]\n",
    "        return 1-(observed / num_vehicles), delay / num_vehicles\n",
    "\n",
    "class Beacon:\n",
    "    def __init__(self):\n",
    "        self.trace_hidden = None\n",
    "    \n",
    "    def update(self, trace_hidden):\n",
    "        self.trace_hidden = copy.deepcopy(trace_hidden)\n",
    "\n",
    "class Vehicle:\n",
    "    def __init__(self):\n",
    "        self.received = 0\n",
    "        self.sent = {}\n",
    "\n",
    "    def step(self):\n",
    "        self.received = 0\n",
    "        for vehicle, lag in self.sent.items():\n",
    "            self.sent[vehicle] += 1\n",
    "        self.sent = {vehicle: lag for vehicle, lag in self.sent.items() if lag < 10}\n",
    "\n",
    "    def receive(self):\n",
    "        self.received += 1\n",
    "    \n",
    "    # initialize the beacon in non-relay mode, select the neighbor with the highest lag to send\n",
    "    # lag: the time since the last message was sent to the neighbor\n",
    "    def non_relay_select(self, neighbors):\n",
    "        max_lag = 0\n",
    "        selected = None\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in self.sent:\n",
    "                selected = neighbor\n",
    "                break\n",
    "            if self.sent[neighbor] > max_lag:\n",
    "                max_lag = self.sent[neighbor]\n",
    "                selected = neighbor\n",
    "        return selected\n",
    "    \n",
    "    def send(self, selected_neighbor):\n",
    "        self.sent[selected_neighbor] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'reward'))\n",
    "\n",
    "class GRU_RL(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRU_RL, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n",
      "***Starting server on port 37577 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (107ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "Simulation ended at time: 1100.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 1.85s\n",
      " TraCI-Duration: 1.60s\n",
      " Real time factor: 593.952\n",
      " UPS: 24395.248380\n",
      "Vehicles: \n",
      " Inserted: 111\n",
      " Running: 52\n",
      " Waiting: 0\n",
      "Statistics (avg of 59):\n",
      " RouteLength: 4182.20\n",
      " Speed: 9.57\n",
      " Duration: 445.41\n",
      " WaitingTime: 15.56\n",
      " TimeLoss: 61.44\n",
      " DepartDelay: 0.48\n",
      "\n",
      "0 0.10869853917662682\n",
      "0.10869853917662682\n",
      "1 0.17454625940681717\n",
      "0.283244798583444\n",
      "2 0.16290393979637008\n",
      "0.44614873837981406\n",
      "3 0.12312970340858786\n",
      "0.5692784417884019\n",
      "4 0.1067286409915892\n",
      "0.6760070827799911\n",
      "5 0.06983178397521027\n",
      "0.7458388667552014\n",
      "6 0.0515493581230633\n",
      "0.7973882248782647\n",
      "7 0.047543160690571046\n",
      "0.8449313855688357\n",
      "8 0.04880478087649402\n",
      "0.8937361664453297\n",
      "9 0.03758300132802125\n",
      "0.931319167773351\n",
      "10 0.02605135015493581\n",
      "0.9573705179282868\n",
      "11 0.014984506418769366\n",
      "0.9723550243470561\n",
      "12 0.013302346170872068\n",
      "0.9856573705179282\n",
      "13 0.008742806551571492\n",
      "0.9944001770694997\n",
      "14 0.004493138556883576\n",
      "0.9988933156263833\n",
      "15 0.0011066843736166445\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# find the maximum action space\n",
    "randomTrips(1000, 1.5)\n",
    "restart(sumoCmd)\n",
    "max_action_spaces = []\n",
    "total_actions = 0\n",
    "\n",
    "total_missing_gain = 0\n",
    "total_delay_gain = 0\n",
    "step = 0\n",
    "\n",
    "\n",
    "while shouldContinueSim():\n",
    "    step += 1\n",
    "    if step > 1100:\n",
    "        close()\n",
    "        break\n",
    "    traci.simulationStep()\n",
    "    ids = traci.vehicle.getIDList()\n",
    "    connectivity, xs, ys = intervehicleConnectivity(800)\n",
    "    # minus the diagonal\n",
    "    action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "    for i, vehicle in enumerate(ids):\n",
    "        # get non-zero indices except the diagonal\n",
    "        non_zero_indices = np.where(action_spaces[i] == 1)[0]\n",
    "        max_action_spaces.append(len(non_zero_indices))\n",
    "\n",
    "# print each action space's proportion\n",
    "cumulative = 0\n",
    "for i in list(set(sorted(max_action_spaces, reverse=True))):\n",
    "    print(i, max_action_spaces.count(i)/len(max_action_spaces))\n",
    "    cumulative += max_action_spaces.count(i)/len(max_action_spaces)\n",
    "    print(cumulative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumoGym(gym.Env):\n",
    "    def __init__(self, sumoCmd, max_action_space, max_steps=1100):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.max_action_space = max_action_space\n",
    "        self.max_steps = max_steps\n",
    "        self.step_counter = 0\n",
    "        self.vehicle_knowledges = {}\n",
    "        self.vehicle_ids = None\n",
    "        self.action_spaces = None\n",
    "        self.vehicle_records = {}\n",
    "        self.xs = None\n",
    "        self.ys = None\n",
    "        self.beacons = {}\n",
    "    \n",
    "    # update the simulation to the next step\n",
    "    def step(self):\n",
    "        # check if the simulation is done\n",
    "        # if done, close the simulation and return True\n",
    "        if self.simDone():\n",
    "            print(\"Simulation is done.\")\n",
    "            traci.close()\n",
    "            return True\n",
    "        # if not done, update the simulation to the next step\n",
    "        else:\n",
    "            traci.simulationStep()\n",
    "            self.step_counter += 1\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            connectivity, self.xs, self.ys = intervehicleConnectivity(800)\n",
    "            # minus the diagonal, as the vehicle cannot send to itself\n",
    "            self.action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "            \n",
    "            for i, vehicle in enumerate(self.vehicle_ids):\n",
    "                # get new knowledge and new records for new vehicles\n",
    "                if vehicle not in self.vehicle_knowledges:\n",
    "                    self.vehicle_knowledges[vehicle] = Knowledges()\n",
    "                if vehicle not in self.vehicle_records:\n",
    "                    self.vehicle_records[vehicle] = Vehicle()\n",
    "                # add observations to the knowledge\n",
    "                self.vehicle_knowledges[vehicle].add_observations(self.vehicle_ids, connectivity[i])\n",
    "                # remove vehicle records if it is not in the simulation\n",
    "            self.vehicle_recodes = {vehicle: record for vehicle, record in self.vehicle_recodes.items() if vehicle in self.vehicle_ids}\n",
    "            for v in self.vehicle_records.keys():\n",
    "                self.vehicle_records[v].step()\n",
    "            return False\n",
    "        \n",
    "    # render the simulation, show the GUI if True\n",
    "    def render(self):\n",
    "        self.show_gui = True\n",
    "\n",
    "    # reset the simulation\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "        while not self.simDone():\n",
    "            self.step()\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            if len(self.vehicle_ids)>1:\n",
    "                break\n",
    "\n",
    "        return self.getCurrentStates()\n",
    "    \n",
    "    def getActionSpaces(self, i):\n",
    "        non_zero_indices = np.where(action_spaces[i] == 1)[0]\n",
    "        # we randomly select the action space if the number of non-zero indices is greater than the max action space\n",
    "        if len(non_zero_indices) > self.max_action_space:\n",
    "            non_zero_indices = np.random.choice(non_zero_indices, self.max_action_space, replace=False)\n",
    "        return non_zero_indices\n",
    "\n",
    "    def nonRelayAct(self, i):\n",
    "        vehicle = self.vehicle_ids[i]\n",
    "        neighbors_i = self.getActionSpaces(i)\n",
    "        neighbors = [self.vehicle_ids[j] for j in neighbors_i]\n",
    "        send_to = self.vehicle_records[vehicle].non_relay_select(neighbors)\n",
    "        if send_to is not None:\n",
    "            self.vehicle_records[send_to].receive()\n",
    "        else:\n",
    "            print(\"No vehicle to send.\")\n",
    "        self.act(i, send_to)\n",
    "    \n",
    "    def act(self, i, selected_index):\n",
    "        vehicle = self.vehicle_ids[i]\n",
    "        receiver = self.vehicle_ids[selected_index]\n",
    "        _, _, missing_gain, delay_gain = self.vehicle_knowledges[receiver].merge_knowledges(self.vehicle_knowledges[vehicle].get_knowledges(), self.vehicle_knowledges[vehicle].get_delays())\n",
    "        reward = self.getRewards(missing_gain, delay_gain)\n",
    "        self.vehicle_recodes[vehicle].send(receiver)\n",
    "        return reward\n",
    "\n",
    "    def getCurrentStates(self):\n",
    "        \"\"\"\n",
    "        function: Get all the states of vehicles, observation space.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for i in range(len(self.action_spaces)):\n",
    "            # pad both xs and ys to the max action space\n",
    "            other_vehicles_xs = (self.xs * self.action_spaces[i])\n",
    "            other_vehicles_ys = (self.ys * self.action_spaces[i])\n",
    "            other_vehicles_xs = F.pad(other_vehicles_xs, (0, self.max_action_space - other_vehicles_xs.size(0)), \"constant\", 0)\n",
    "            other_vehicles_ys = F.pad(other_vehicles_ys, (0, self.max_action_space - other_vehicles_ys.size(0)), \"constant\", 0)\n",
    "            states.append(torch.cat((other_vehicles_xs, other_vehicles_ys)).view(-1))\n",
    "        states = torch.stack(states)\n",
    "        return states, self.vehicle_ids\n",
    "\n",
    "        \n",
    "    def getRewards(self, missing_reduction, delay_reduction, missing_coefficient=2, delay_coefficient=3):\n",
    "        return missing_reduction * missing_coefficient + delay_reduction * delay_coefficient\n",
    "        \n",
    "    def simDone(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter <= self.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended at time: 66.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 28.09s\n",
      " TraCI-Duration: 0.15s\n",
      " Real time factor: 2.34942\n",
      " UPS: 9.824861\n",
      "Vehicles: \n",
      " Inserted: 8 (Loaded: 25)\n",
      " Running: 8\n",
      " Waiting: 0\n",
      "Statistics (avg of 0):\n",
      " RouteLength: 0.00\n",
      " Speed: 0.00\n",
      " Duration: 0.00\n",
      " WaitingTime: 0.00\n",
      " TimeLoss: 0.00\n",
      " DepartDelay: 0.00\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 40897 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (103ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 2\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = max_action_space\n",
    "# Get the number of state observations\n",
    "env = SumoGym(sumoCmd, max_action_space)\n",
    "state, v_ids = env.reset()\n",
    "n_observations = max_action_space * 2\n",
    "\n",
    "policy_net = GRU_RL(input_size = n_observations, hidden_size=128, output_size=n_actions, num_layers=5).to(device)\n",
    "target_net = GRU_RL(input_size = n_observations, hidden_size=128, output_size=n_actions, num_layers=5).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state, hidden=None):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    with torch.no_grad():\n",
    "        out, out_hidden = policy_net(state.unsqueeze(0), hidden)\n",
    "        if sample > eps_threshold:\n",
    "                return out.max(1).indices.view(1, 1), out_hidden\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long), out_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "    #                                       batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "    #                                             if s is not None])\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    reward_batch = torch.stack(batch.reward)\n",
    "    print(\"State batch: \", state_batch)\n",
    "    print(batch.state)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values, hiddens = policy_net(state_batch).gather(1, action_batch)\n",
    "    print(\"State action values: \", state_action_values)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended at time: 66.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 46.40s\n",
      " TraCI-Duration: 0.15s\n",
      " Real time factor: 1.42244\n",
      " UPS: 5.948404\n",
      "Vehicles: \n",
      " Inserted: 8 (Loaded: 25)\n",
      " Running: 8\n",
      " Waiting: 0\n",
      "Statistics (avg of 0):\n",
      " RouteLength: 0.00\n",
      " Speed: 0.00\n",
      " Duration: 0.00\n",
      " WaitingTime: 0.00\n",
      " TimeLoss: 0.00\n",
      " DepartDelay: 0.00\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 51061 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (106ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "State batch:  tensor([[   0.0000,    0.0000, 2676.1895,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000, 2317.1985,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000, 3274.7283,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000, 1796.8612,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000]],\n",
      "       device='cuda:0')\n",
      "(tensor([   0.0000,    0.0000, 2676.1895,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000, 2317.1985,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "       device='cuda:0'), tensor([   0.0000, 3274.7283,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000, 1796.8612,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "           0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "       device='cuda:0'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'gather'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m memory\u001b[38;5;241m.\u001b[39mpush(state, action, reward)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     34\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# columns of actions taken. These are the actions which would've been taken\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# for each batch state according to policy_net\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState action values: \u001b[39m\u001b[38;5;124m\"\u001b[39m, state_action_values)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# on the \"older\" target_net; selecting their best reward with max(1).values\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# This is merged based on the mask, such that we'll have either the expected\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# state value or 0 in case the state was final.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'gather'"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    states, v_ids = env.reset()\n",
    "    states = states.to(device)\n",
    "    done = False\n",
    "    hiddens = {}\n",
    "    while not done:\n",
    "        done = env.step()\n",
    "        states, v_ids = env.getCurrentStates()\n",
    "        for i in range(states.size(0)):\n",
    "            curr_v = v_ids[i]\n",
    "            state = states[i].to(device)\n",
    "            if max(state) == 0:\n",
    "                continue\n",
    "            if curr_v not in hiddens:\n",
    "                action, v_hidden = select_action(state)\n",
    "            else:\n",
    "                action, v_hidden = select_action(state, hiddens[curr_v])\n",
    "            hiddens[curr_v] = v_hidden\n",
    "            if action >= len(env.action_spaces[i]) or env.action_spaces[i][action.item()] == 0:\n",
    "                reward = torch.tensor([0], device=device)\n",
    "            else:\n",
    "                reward = env.act(v_ids[i], action.item())\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "            memory.push(state, action, reward)\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            optimize_model()\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
