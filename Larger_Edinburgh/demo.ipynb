{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from utils import *\n",
    "import copy\n",
    "import gym\n",
    "import random\n",
    "from Models import GRUModel, SAGE_GDQN_Attention, JK_SAGE_GDQN_Attention\n",
    "from Envs import *\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from Knowledges import *\n",
    "from SentHistory import *\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Batch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "# sumoBinary = checkBinary('sumo-gui')\n",
    "sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectivity(xs,ys, threshold=800):\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        connectivity = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    connectivity = connectivity - torch.diag(torch.diag(connectivity))\n",
    "    return connectivity, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Incomplete_Routing_Gym(gym.Env):\n",
    "    def __init__(self, sumoCmd, model, max_steps=1100, n_nodes=57, max_routing_steps=30, max_n_neighbors=6):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.n_nodes = n_nodes\n",
    "        self.start_node = None\n",
    "        self.end_node = None\n",
    "        self.current_node = None\n",
    "        self.node_features = None\n",
    "        self.adj_matrix = None\n",
    "        self.edge_index = None\n",
    "        self.hop_thresh = None\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        self.min_n_hops = None\n",
    "        self.end_node_indicator = torch.zeros(n_nodes)\n",
    "        self.max_routing_steps = max_routing_steps\n",
    "        self.n_hop_matrix = None\n",
    "        self.neighbors_indicator = None\n",
    "        self.action_space = None\n",
    "        self.to_remove_indices = None\n",
    "        self.prunned_adj_matrix = None\n",
    "        self.prunned_n_hop_matrix = None\n",
    "        self.state = None\n",
    "        self.max_n_neighbors = max_n_neighbors\n",
    "        self.ids = None\n",
    "        self.vehicle_knowledges = {}\n",
    "        self.vehicle_records = {}\n",
    "        self.GRUModel = model\n",
    "        self.GRUModel.eval()\n",
    "\n",
    "        self.cached_states = {}\n",
    "\n",
    "        self.xs = None\n",
    "        self.ys = None\n",
    "        self.start_x = None\n",
    "        self.start_y = None\n",
    "        self.end_x = None\n",
    "        self.end_y = None\n",
    "\n",
    "        self.total_missing_ratio = 0\n",
    "        self.total_error = 0\n",
    "        self.total_num_graphs = 0\n",
    "\n",
    "        self.delay_distribution = []\n",
    "\n",
    "        self.observed_adj_matrix = None\n",
    "        self.observed_n_hop_matrix = None\n",
    "\n",
    "        self.trajectory_dict = {}\n",
    "\n",
    "\n",
    "        self.load_all_trajectory()\n",
    "\n",
    "        self.destination_unknown = False\n",
    "\n",
    "    \n",
    "\n",
    "    def load_all_trajectory(self):\n",
    "        self.checkpoints_dict = get_planned_path()\n",
    "        checkpoints = list(self.checkpoints_dict.values())\n",
    "        checkpoints = torch.tensor(checkpoints).float() / 10\n",
    "        position_df = pd.read_csv('trajectory_time.csv')\n",
    "        position_df.set_index('Unnamed: 0', inplace=True)\n",
    "        position_array = position_df.to_numpy()\n",
    "        sequence_length = position_df.shape[1] // 2\n",
    "        tensor_list = []\n",
    "\n",
    "        for row in position_array:\n",
    "            reshaped_tensor = torch.tensor(row.reshape(sequence_length, 2))\n",
    "            tensor_list.append(reshaped_tensor)\n",
    "\n",
    "        all_trajectories_tensor = torch.stack(tensor_list).float() / 10\n",
    "\n",
    "        next_checkpoint = torch.zeros_like(all_trajectories_tensor)\n",
    "        next_next_checkpoint = torch.zeros_like(all_trajectories_tensor)\n",
    "        checkpoints_pad_1 = F.pad(checkpoints, (0, 0, 1, 0))\n",
    "        for i in range(all_trajectories_tensor.shape[1]):\n",
    "            _, min_indices = project_to_nearest(all_trajectories_tensor[:, i], checkpoints)\n",
    "            next_checkpoint[:, i] = checkpoints_pad_1[range(checkpoints.shape[0]), min_indices+1]\n",
    "            next_next_checkpoint[:, i] = checkpoints_pad_1[range(checkpoints.shape[0]), min_indices+2]\n",
    "\n",
    "        all_trajectories_tensor = torch.cat((all_trajectories_tensor, next_checkpoint, next_next_checkpoint), dim=2)\n",
    "\n",
    "        # convert all_trajectories_tensor to a dictionary, where keys are the vehicle ids\n",
    "        for i, n in enumerate(position_df.index):\n",
    "            self.trajectory_dict[str(n)] = all_trajectories_tensor[i]\n",
    "            self.checkpoints_dict[str(n)] = checkpoints[i]\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(self.sumoCmd)\n",
    "        self.step_counter = traci.simulation.getTime()\n",
    "\n",
    "        while self.step_counter < 400:\n",
    "            self.simstep_with_sync()\n",
    "\n",
    "\n",
    "    def node_pruning(self):\n",
    "        self.prunned_adj_matrix = copy.deepcopy(self.observed_adj_matrix)\n",
    "        self.prunned_n_hop_matrix = copy.deepcopy(self.observed_n_hop_matrix)\n",
    "        neighbor_indices = np.where(self.observed_adj_matrix[self.current_node] == 1)[0]\n",
    "        if len(neighbor_indices) >= self.max_n_neighbors:\n",
    "            two_hop_neighbours_indices = np.where(self.observed_n_hop_matrix[self.current_node] == 2)[0]\n",
    "            two_hop_neighbours_mask = (self.observed_n_hop_matrix[self.current_node] == 2).type(torch.int)\n",
    "            # direct neighbours connectivities with two hop neighbours\n",
    "            neighbour_dict = {}\n",
    "            for neighbour_index in neighbor_indices:\n",
    "                neighbour_dict[neighbour_index] = two_hop_neighbours_indices[np.where(self.observed_adj_matrix[neighbour_index][two_hop_neighbours_indices] == 1)[0]]\n",
    "            # sort by the number of two hop neighbours\n",
    "            neighbour_dict = dict(sorted(neighbour_dict.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "\n",
    "            self.to_remove_indices = []\n",
    "            action_space = 0\n",
    "            for neighbour_index, two_hop_neighbours_indices in neighbour_dict.items():\n",
    "                mask_sum_before = torch.sum(two_hop_neighbours_mask)\n",
    "                two_hop_neighbours_mask[two_hop_neighbours_indices] = 0\n",
    "                mask_sum_after = torch.sum(two_hop_neighbours_mask)\n",
    "                if mask_sum_after < mask_sum_before:\n",
    "                    action_space += 1\n",
    "                else:\n",
    "                    self.to_remove_indices.append(neighbour_index)\n",
    "            if action_space < self.max_n_neighbors:\n",
    "                self.to_remove_indices = random.sample(self.to_remove_indices, len(self.to_remove_indices) - (self.max_n_neighbors - action_space))\n",
    "            self.prunned_adj_matrix[self.to_remove_indices, :] = 0\n",
    "            self.prunned_adj_matrix[:, self.to_remove_indices] = 0\n",
    "            self.prunned_n_hop_matrix[self.to_remove_indices, :] = -100\n",
    "            self.prunned_n_hop_matrix[:, self.to_remove_indices] = -100\n",
    "        self.prunned_n_hop_matrix = self.prunned_n_hop_matrix - torch.diag(torch.diag(self.prunned_n_hop_matrix))\n",
    "        self.prunned_adj_matrix = self.prunned_adj_matrix - torch.diag(torch.diag(self.prunned_adj_matrix))\n",
    "\n",
    "\n",
    "    def next_episode(self, refresh=False):\n",
    "        self.destination_unknown = False\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        if not refresh:\n",
    "            self.simstep_with_sync()\n",
    "            self.adj_matrix = F.pad(self.adj_matrix, (0, self.n_nodes - self.adj_matrix.size(0), \n",
    "                                                  0, self.n_nodes - self.adj_matrix.size(1)), \"constant\", 0)\n",
    "            self.n_hop_matrix = F.pad(self.n_hop_matrix, (0, self.n_nodes - self.n_hop_matrix.size(0), \n",
    "                                                      0, self.n_nodes - self.n_hop_matrix.size(1)), \"constant\", -100)\n",
    "        self.select_start_end_nodes()\n",
    "        self.current_node = self.start_node\n",
    "        self.get_state()\n",
    "        return copy.deepcopy(self.state)\n",
    "\n",
    "    def simstep_with_sync(self):\n",
    "        traci.simulationStep()\n",
    "        self.cached_states = {}\n",
    "        self.step_counter = int(traci.simulation.getTime())\n",
    "        self.adj_matrix, self.xs, self.ys = intervehicleConnectivity_xs_ys(800)\n",
    "        self.n_hop_matrix = bfs_distance(self.adj_matrix)\n",
    "        self.ids = traci.vehicle.getIDList()\n",
    "        action_spaces = self.adj_matrix - torch.diag(torch.diag(self.adj_matrix))\n",
    "\n",
    "        for i, vehicle in enumerate(self.ids):\n",
    "            if vehicle not in self.vehicle_knowledges:\n",
    "                self.vehicle_knowledges[vehicle] = Knowledges()\n",
    "            if vehicle not in self.vehicle_records:\n",
    "                self.vehicle_records[vehicle] = Vehicle()\n",
    "            self.vehicle_knowledges[vehicle].add_observations(self.ids, self.adj_matrix[i])\n",
    "        \n",
    "        for i, vehicle in enumerate(self.ids):\n",
    "            non_zero_indices = torch.where(action_spaces[i] == 1)[0]\n",
    "            neighbors = [self.ids[j] for j in non_zero_indices] \n",
    "            if len(neighbors) > 0:\n",
    "                select_neighbour = self.vehicle_records[vehicle].select(neighbors)\n",
    "                if select_neighbour is None:\n",
    "                    continue\n",
    "                self.vehicle_records[vehicle].send(select_neighbour)\n",
    "                self.vehicle_knowledges[select_neighbour].merge_knowledges(self.vehicle_knowledges[vehicle].get_knowledges(), self.vehicle_knowledges[vehicle].get_delays())\n",
    "\n",
    "                self.vehicle_records[select_neighbour].send(vehicle)\n",
    "                self.vehicle_knowledges[vehicle].merge_knowledges(self.vehicle_knowledges[select_neighbour].get_knowledges(), self.vehicle_knowledges[select_neighbour].get_delays())\n",
    "            \n",
    "        for vehicle in self.vehicle_records.values():\n",
    "            vehicle.step()\n",
    "\n",
    "        \n",
    "    def select_start_end_nodes(self):\n",
    "        self.hop_thresh = min(self.n_hop_matrix.max(), 5)\n",
    "        starts, ends = torch.where(self.hop_thresh == self.n_hop_matrix)\n",
    "        starts = starts.tolist()\n",
    "        ends = ends.tolist()\n",
    "        self.start_node, self.end_node = random.choice(list(zip(starts, ends)))\n",
    "        # minimal number of hops between start and end nodes\n",
    "        self.min_n_hops = self.n_hop_matrix[self.start_node, self.end_node]\n",
    "\n",
    "        self.start_x = self.xs[self.start_node]\n",
    "        self.start_y = self.ys[self.start_node]\n",
    "        self.end_x = self.xs[self.end_node]\n",
    "        self.end_y = self.ys[self.end_node]\n",
    "\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.current_node in self.cached_states:\n",
    "            self.state = copy.deepcopy(self.cached_states[self.current_node])\n",
    "        self.compute_state()\n",
    "        self.cached_states[self.current_node] = copy.deepcopy(self.state)\n",
    "    \n",
    "\n",
    "    def compute_state(self):\n",
    "        current_vehicle_knowledge = self.vehicle_knowledges[self.ids[self.current_node]].get_knowledges()\n",
    "        current_vehicle_delay = self.vehicle_knowledges[self.ids[self.current_node]].get_delays()\n",
    "        observed_vehicle_xs = []\n",
    "        observed_vehicle_ys = []\n",
    "        for vehicle in self.ids:\n",
    "            if current_vehicle_delay[vehicle] == 0:\n",
    "                observed_vehicle_xs.append(self.xs[self.ids.index(vehicle)])\n",
    "                observed_vehicle_ys.append(self.ys[self.ids.index(vehicle)])\n",
    "            elif current_vehicle_delay[vehicle] >= 10 or sum(current_vehicle_knowledge[vehicle]) <= 3:\n",
    "                observed_vehicle_xs.append(0)\n",
    "                observed_vehicle_ys.append(0)\n",
    "            else:\n",
    "                x, y = self.estimate_vehicle_positions(vehicle, current_vehicle_knowledge[vehicle], current_vehicle_delay[vehicle])\n",
    "                observed_vehicle_xs.append(x)\n",
    "                observed_vehicle_ys.append(y)\n",
    "        \n",
    "        self.observed_adj_matrix, xs, ys = connectivity(observed_vehicle_xs, observed_vehicle_ys)\n",
    "        # set connectivity to 0 if the vehicle is not observed\n",
    "        not_observed_indices = torch.where(xs == -1)[0]\n",
    "        self.observed_adj_matrix[not_observed_indices, :] = 0\n",
    "        self.observed_adj_matrix[:, not_observed_indices] = 0\n",
    "\n",
    "        self.observed_adj_matrix = F.pad(self.observed_adj_matrix, (0, self.n_nodes - self.observed_adj_matrix.size(0), \n",
    "                                                  0, self.n_nodes - self.observed_adj_matrix.size(1)), \"constant\", 0)\n",
    "        # set the neighbouring connectivity to the true connectivity\n",
    "        non_neighbour_indices = torch.where(self.adj_matrix[self.current_node] == 0)[0]\n",
    "        self.observed_adj_matrix[self.current_node, non_neighbour_indices] = 0\n",
    "        self.observed_adj_matrix[non_neighbour_indices, self.current_node] = 0\n",
    "\n",
    "        if xs[self.end_node] == 0 and ys[self.end_node] == 0:\n",
    "            self.destination_unknown = True\n",
    "\n",
    "        norm_xs = (xs - self.end_x) / (self.start_x - self.end_x)\n",
    "        norm_ys = (ys - self.end_y) / (self.start_y - self.end_y)\n",
    "        # set the norm_xs and norm_ys to 0 if the vehicle is not observed\n",
    "        norm_xs[not_observed_indices] = 0\n",
    "        norm_ys[not_observed_indices] = 0\n",
    "        # padding the matrices to the same size\n",
    "        norm_xs = F.pad(norm_xs, (0, 0, 0, self.n_nodes - norm_xs.size(0)), \"constant\", 0)\n",
    "        norm_ys = F.pad(norm_ys, (0, 0, 0, self.n_nodes - norm_ys.size(0)), \"constant\", 0)\n",
    "        self.observed_n_hop_matrix = bfs_distance(self.observed_adj_matrix)\n",
    "        self.node_pruning()\n",
    "        edge_index = dense_to_sparse(self.prunned_adj_matrix)[0]\n",
    "\n",
    "        curr_node_indicator = torch.zeros(self.n_nodes)\n",
    "        curr_node_indicator[self.current_node] = 1\n",
    "        distances = self.prunned_n_hop_matrix[self.end_node]\n",
    "        distances[distances == -100] = 8\n",
    "        distances[distances > 7] = 7\n",
    "        one_hot_distances = F.one_hot(distances.long(), num_classes=8).type(torch.float32)\n",
    "        neighbour_indicator = self.prunned_adj_matrix[self.current_node]\n",
    "        uncertainties_delay = torch.tensor([current_vehicle_delay[vehicle] for vehicle in self.ids]).unsqueeze(1) / 10\n",
    "        uncertainties_delay = F.pad(uncertainties_delay, (0, 0, 0, self.n_nodes - uncertainties_delay.size(0)), \"constant\", 1)\n",
    "        node_features = torch.cat((one_hot_distances, uncertainties_delay, self.end_node_indicator.unsqueeze(1), \n",
    "                                   curr_node_indicator.unsqueeze(1), neighbour_indicator.unsqueeze(1), \n",
    "                                   norm_xs, norm_ys), dim=1)\n",
    "        self.state = Data(x=node_features, edge_index=edge_index).to(device)\n",
    "\n",
    "\n",
    "    def act(self, neighbor_index):\n",
    "        self.routing_steps += 1\n",
    "        neighbors = torch.where(self.prunned_adj_matrix[self.current_node] == 1)[0]\n",
    "        valid_action_size = len(neighbors)\n",
    "        if valid_action_size <= neighbor_index:\n",
    "            self.routing_done = self.routing_steps >= self.max_routing_steps\n",
    "            if self.routing_done:\n",
    "                return self.state, torch.tensor(-1).to(device), self.routing_done\n",
    "            return self.state, torch.tensor(-0.15).to(device), self.routing_done\n",
    "        else:\n",
    "            next_hop = neighbors[neighbor_index]\n",
    "            reward = self.compute_reward(next_hop)\n",
    "            self.current_node = next_hop\n",
    "            self.get_state()\n",
    "            return self.state, torch.tensor(reward).to(device), self.routing_done\n",
    "\n",
    "    \n",
    "    def estimate_vehicle_positions(self, vehicle, observation_history, delay):\n",
    "        last_seen_time = int(self.step_counter - delay)\n",
    "        first_one_index = int(observation_history.index(1))\n",
    "        sequence_length = min(len(observation_history) - first_one_index, 20)\n",
    "        # these last seen value will be used to calibrate the position of the vehicle\n",
    "        # we set the last seen value to 50, 50, as the autoregressive model is trained on the normalized values\n",
    "        last_seen_x = self.trajectory_dict[vehicle][last_seen_time][0].item()\n",
    "        last_seen_y = self.trajectory_dict[vehicle][last_seen_time][1].item()\n",
    "        modif = (torch.tensor([last_seen_x, last_seen_y])-torch.tensor([50, 50])).repeat(1, 3)\n",
    "\n",
    "        masks = torch.tensor(observation_history).unsqueeze(0).unsqueeze(2).repeat(1, 1, 6)\n",
    "\n",
    "        paths = self.checkpoints_dict[vehicle].unsqueeze(0)\n",
    "        inputs = copy.deepcopy(self.trajectory_dict[vehicle][self.step_counter-sequence_length+1:self.step_counter+1].unsqueeze(0))\n",
    "        inputs -= modif\n",
    "        masks = masks[:, -sequence_length:, :]\n",
    "        modified_x, modified_y = self.GRU_inference(inputs.to(device), masks.to(device), paths.to(device))\n",
    "        x, y = (modified_x + modif[0][0]).item() * 10, (modified_y + modif[0][1]).item() * 10\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "    def GRU_inference(self, inputs, masks, paths):\n",
    "        with torch.no_grad():\n",
    "            hidden = None\n",
    "            seq_len = inputs.size(1)\n",
    "            current_input = inputs[:, 0, :].unsqueeze(1)\n",
    "            for t in range(1, seq_len):\n",
    "                prediction, hidden = self.GRUModel(current_input, hidden)\n",
    "                projection_with_checkpoints = project_to_nearest_with_checkpoints(prediction, paths)\n",
    "                current_input = (projection_with_checkpoints * (1-masks[:, t, :]) + inputs[:, t, :] * (masks[:, t, :])).unsqueeze(1)\n",
    "        return current_input[0,0,0], current_input[0,0,1]\n",
    "    \n",
    "\n",
    "    def compute_reward(self, next_hop):\n",
    "        if self.routing_steps >= self.max_routing_steps:\n",
    "            self.routing_done = True\n",
    "            return -1\n",
    "        elif self.adj_matrix[self.current_node, self.end_node] == 1:\n",
    "            self.routing_done = True\n",
    "            return (self.min_n_hops / self.routing_steps)\n",
    "        elif self.n_hop_matrix[self.current_node, self.end_node] > self.n_hop_matrix[next_hop, self.end_node]:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return -0.15\n",
    "\n",
    "\n",
    "    def get_action_mask(self):\n",
    "        action_mask = copy.deepcopy(self.prunned_adj_matrix[self.current_node])\n",
    "        action_mask = F.pad(action_mask, (0, self.n_nodes - action_mask.size(0)), \"constant\", 0).to(device)\n",
    "        return action_mask\n",
    "\n",
    "\n",
    "    def sim_done(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter < self.max_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the GRL on Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.95\n",
    "EPS_START = 0.0\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 5000\n",
    "TAU = 0.05\n",
    "LR = 0.001\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('data', 'action', 'next_state', 'reward', 'shuffle_indices'))\n",
    "\n",
    "\n",
    "def select_action(data, action_mask, shuffle_indices):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            valid_action_size = len(torch.where(action_mask == 1)[0])\n",
    "            values = policy_net(data, shuffle_indices)[:valid_action_size]\n",
    "            return torch.tensor([values.max(1).indices.item()], device=device), True\n",
    "    else:\n",
    "        valid_size = len(torch.where(action_mask == 1)[0])\n",
    "        return torch.randint(0, valid_size, (1,), device=device), False\n",
    "\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = Batch.from_data_list([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    data_batch = Batch.from_data_list(batch.data)\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    reward_batch = torch.concat(batch.reward)\n",
    "    shuffle_indices_batch = torch.stack(batch.shuffle_indices)\n",
    "\n",
    "    state_action_values = policy_net(data_batch, shuffle_indices_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation ended at time: 428.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 378.10s\n",
      " TraCI-Duration: 340.39s\n",
      " Real time factor: 1.13198\n",
      " UPS: 25.369151\n",
      "Vehicles: \n",
      " Inserted: 48 (Loaded: 70)\n",
      " Running: 40\n",
      " Waiting: 0\n",
      "Statistics (avg of 8):\n",
      " RouteLength: 2128.15\n",
      " Speed: 9.72\n",
      " Duration: 228.00\n",
      " WaitingTime: 8.12\n",
      " TimeLoss: 32.02\n",
      " DepartDelay: 0.29\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 59397 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (176ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "Simulation ended at time: 1557.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 0.69s\n",
      " TraCI-Duration: 0.45s\n",
      " Real time factor: 2246.75\n",
      " UPS: 66424.242424\n",
      "Vehicles: \n",
      " Inserted: 89\n",
      " Running: 0\n",
      " Waiting: 0\n",
      "Statistics (avg of 89):\n",
      " RouteLength: 4885.02\n",
      " Speed: 9.58\n",
      " Duration: 517.21\n",
      " WaitingTime: 17.72\n",
      " TimeLoss: 70.09\n",
      " DepartDelay: 0.47\n",
      "\n",
      " Retrying in 1 seconds\n",
      "***Starting server on port 43461 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (155ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n"
     ]
    }
   ],
   "source": [
    "n_nodes = 57\n",
    "model = GRUModel(input_size=6, hidden_size=256, num_layers=2, output_size=2).to(device)\n",
    "# load the model\n",
    "model.load_state_dict(torch.load('models/gru_trajectory_prediction.pth'))\n",
    "model.eval()\n",
    "env = Incomplete_Routing_Gym(sumoCmd, model)\n",
    "max_n_neighbors = 6\n",
    "\n",
    "policy_net = SAGE_GDQN_Attention(in_channels=14, n_nodes=n_nodes, max_n_neighbors=max_n_neighbors)\n",
    "target_net = SAGE_GDQN_Attention(in_channels=14, n_nodes=n_nodes, max_n_neighbors=max_n_neighbors)\n",
    "# load the model\n",
    "policy_net.load_state_dict(torch.load('models/SAGE_uncertain_policy.pth'))\n",
    "target_net.load_state_dict(torch.load('models/SAGE_uncertain_target.pth'))\n",
    "policy_net.to(device)\n",
    "target_net.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(1)\n",
    "steps_done = 0\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(env, state):\n",
    "    xs = env.xs / 10\n",
    "    ys = env.ys / 10\n",
    "    start_x = env.start_x.to(device)\n",
    "    start_y = env.start_y.to(device)\n",
    "    end_x = env.end_x.to(device)\n",
    "    end_y = env.end_y.to(device)\n",
    "    # get all vehicles' positions in state\n",
    "    estimated_xs = (state.x[:, -2] * start_x + (1 - state.x[:, -2]) * end_x) / 10\n",
    "    estimated_ys = (state.x[:, -1] * start_y + (1 - state.x[:, -1]) * end_y) / 10\n",
    "    adjacency_matrix = env.prunned_adj_matrix\n",
    "\n",
    "    current = None\n",
    "    destination = None\n",
    "\n",
    "    img = np.zeros((700, 700, 3), dtype=np.uint8)\n",
    "    for i in range(len(xs)):\n",
    "        x = int(xs[i].item())\n",
    "        y = int(ys[i].item())\n",
    "\n",
    "        if i == env.current_node:\n",
    "            current = (x, y)\n",
    "        elif i == env.end_node:\n",
    "            destination = (x, y)\n",
    "        else:\n",
    "            cv2.circle(img, (x, y), 12, (255/3, 255/3, 255/3), -1)\n",
    "\n",
    "        x = int(estimated_xs[i].item())\n",
    "        y = int(estimated_ys[i].item())\n",
    "        cv2.circle(img, (x, y), 4, (255, 255, 255), -1)\n",
    "\n",
    "    for i in range(len(adjacency_matrix)):\n",
    "        for j in range(i, len(adjacency_matrix)):  # use range(i, ...) to avoid duplicate edges\n",
    "            if adjacency_matrix[i, j] != 0:\n",
    "                # Get the positions of the nodes\n",
    "                pt1 = (int(estimated_xs[i]), int(estimated_ys[i]))\n",
    "                pt2 = (int(estimated_xs[j]), int(estimated_ys[j]))\n",
    "                # Draw the edge\n",
    "                cv2.line(img, pt1, pt2, color=(100, 100, 100), thickness=1)\n",
    "\n",
    "    cv2.circle(img, current, 10, (0, 0, 255), -1)\n",
    "    cv2.circle(img, destination, 10, (255, 0, 0), -1)\n",
    "    # put text\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(img, 'Current', (current[0]-30, current[1]-20), font, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(img, 'Destination', (destination[0]-42, destination[1]-20), font, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n",
    "    return copy.deepcopy(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env=None):\n",
    "    fps = 30\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter('routing.mp4', fourcc, fps, (700, 700))\n",
    "    time = 0.3\n",
    "    for i in range(20):\n",
    "        if i != 0:\n",
    "            if skip:\n",
    "                text = 'Failed'\n",
    "            elif timeout:\n",
    "                text = 'Timeout'\n",
    "            else:\n",
    "                text = 'Done'\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 2, 2)[0]\n",
    "            text_x = (700 - text_size[0]) // 2\n",
    "            text_y = (700 - text_size[1]) // 2\n",
    "            cv2.putText(img, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 100, 100), 2, cv2.LINE_AA)\n",
    "            for _ in range(int(1.2 * fps)):\n",
    "                video_writer.write(img)\n",
    "        for j in range(10):\n",
    "            state = env.next_episode()\n",
    "        \n",
    "        routing_done = False\n",
    "        skip = False\n",
    "        timeout = False\n",
    "        while not routing_done:\n",
    "            img = plot(env, state)\n",
    "            # cv2.imshow('Routing', img)\n",
    "            action_mask = env.get_action_mask()\n",
    "            for _ in range(int(time * fps)):\n",
    "                video_writer.write(img)\n",
    "            if env.destination_unknown:\n",
    "                skip = True\n",
    "                break\n",
    "            shuffle_indices = torch.tensor([0, 1, 2, 3, 4, 5])\n",
    "            action, use_policy = select_action(state, action_mask, shuffle_indices)\n",
    "            node_features, reward, routing_done = env.act(action.item())\n",
    "            if reward < -0.2:\n",
    "                timeout = True\n",
    "\n",
    "            routing_done = routing_done\n",
    "            if routing_done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = node_features\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28575/2091530876.py:281: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return self.state, torch.tensor(reward).to(device), self.routing_done\n"
     ]
    }
   ],
   "source": [
    "test(env)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
