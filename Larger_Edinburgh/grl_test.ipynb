{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import copy\n",
    "from itertools import count\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "# sumoBinary = checkBinary('sumo-gui')\n",
    "sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervehicleConnectivity(threshold = None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for vehicle in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(vehicle)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        connectivity = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    return connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n"
     ]
    }
   ],
   "source": [
    "def randomTrips(dur=1000, density=12):\n",
    "    os.system(\"python $SUMO_HOME/tools/randomTrips.py -n config/osm.net.xml.gz -r config/osm.passenger.trips.xml -e \" + str(dur) + \" -l --insertion-density=\" + str(density))\n",
    "\n",
    "def shouldContinueSim():\n",
    "    numVehicles = traci.simulation.getMinExpectedNumber()\n",
    "    return True if numVehicles > 0 else False\n",
    "\n",
    "def restart(sumoCmd):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "def close():\n",
    "    traci.close()\n",
    "\n",
    "randomTrips(800, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_distance(adj_matrix):\n",
    "    n_hop_matrix = torch.ones_like(adj_matrix) * 100\n",
    "    for start_node in range(adj_matrix.size(0)):\n",
    "        visited = [0] * adj_matrix.size(0)\n",
    "        queue = deque([(start_node, 0)])\n",
    "        visited[start_node] = True\n",
    "        \n",
    "        while queue:\n",
    "            current_node, current_dist = queue.popleft()\n",
    "            \n",
    "            for neighbor, connected in enumerate(adj_matrix[current_node]):\n",
    "                if connected and not visited[neighbor]:\n",
    "                    queue.append((neighbor, current_dist + 1))\n",
    "                    visited[neighbor] = True\n",
    "                    n_hop_matrix[start_node, neighbor] = current_dist + 1\n",
    "    return n_hop_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutingGym(gym.Env):\n",
    "    def __init__(self, sumoCmd, max_steps=1100, n_nodes=57, max_routing_steps=100):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.n_nodes = n_nodes\n",
    "        self.vehicle_ids = None\n",
    "        self.start_node = None\n",
    "        self.end_node = None\n",
    "        self.current_node = None\n",
    "        self.node_features = None\n",
    "        self.adj_matrix = None\n",
    "        self.edge_index = None\n",
    "        self.hop_thresh = None\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        self.min_n_hops = None\n",
    "        self.end_node_indicator = torch.zeros(n_nodes)\n",
    "        self.max_routing_steps = max_routing_steps\n",
    "        self.n_hop_matrix = None\n",
    "        self.neighbors_indicator = None\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        while self.step_counter < 400:\n",
    "            traci.simulationStep()\n",
    "            self.step_counter += 1\n",
    "\n",
    "    def step(self):\n",
    "        # traci.simulationStep()\n",
    "        self.end_node_indicator = torch.zeros(self.n_nodes)\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        self.step_counter += 1\n",
    "        self.vehicle_ids = traci.vehicle.getIDList()\n",
    "        self.adj_matrix = intervehicleConnectivity(800)\n",
    "        self.adj_matrix = self.adj_matrix - torch.eye(self.adj_matrix.size(0))\n",
    "        self.select_start_end_nodes()\n",
    "        self.current_node = self.start_node\n",
    "        self.adj_matrix = F.pad(self.adj_matrix, (0, self.n_nodes - self.adj_matrix.size(0), 0, self.n_nodes - self.adj_matrix.size(1)), \"constant\", 0)\n",
    "        self.n_hop_matrix = F.pad(self.n_hop_matrix, (0, self.n_nodes - self.n_hop_matrix.size(0), 0, self.n_nodes - self.n_hop_matrix.size(1)), \"constant\", 100)\n",
    "        # set diagonal to 0\n",
    "        self.n_hop_matrix = self.n_hop_matrix - torch.diag(torch.diag(self.n_hop_matrix))\n",
    "        self.edge_index, _ = dense_to_sparse(self.adj_matrix)\n",
    "        current_node_indicators = torch.zeros(self.n_nodes)\n",
    "        current_node_indicators[self.current_node] = 1\n",
    "        self.end_node_indicator[self.end_node] = 1\n",
    "        self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "        self.node_features = torch.stack((current_node_indicators, \n",
    "                                          self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "        \n",
    "        return self.node_features.to(device)\n",
    "\n",
    "    def select_start_end_nodes(self):\n",
    "        self.n_hop_matrix = bfs_distance(self.adj_matrix)\n",
    "        self.hop_thresh = min(self.n_hop_matrix.max(), 2)\n",
    "        starts, ends = torch.where(self.hop_thresh <= self.n_hop_matrix)\n",
    "        starts = starts.tolist()\n",
    "        ends = ends.tolist()\n",
    "        self.start_node, self.end_node = random.choice(list(zip(starts, ends)))\n",
    "        # minimal number of hops between start and end nodes\n",
    "        self.min_n_hops = self.n_hop_matrix[self.start_node, self.end_node]\n",
    "\n",
    "    def act(self, neighbor_index):\n",
    "        self.routing_steps += 1\n",
    "        neighbors = torch.where(self.adj_matrix[self.current_node] == 1)[0]\n",
    "        valid_action_size = len(neighbors)\n",
    "        if valid_action_size <= neighbor_index:\n",
    "            if self.node_features.device != device:\n",
    "                self.node_features = self.node_features.to(device)\n",
    "            return self.node_features, torch.tensor(0).to(device), False\n",
    "        else:\n",
    "            next_hop = neighbors[neighbor_index]\n",
    "            reward = self.compute_reward(next_hop)\n",
    "            self.current_node = next_hop\n",
    "            curr_node_indicators = torch.zeros(self.n_nodes)\n",
    "            curr_node_indicators[self.current_node] = 1\n",
    "            self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "            self.node_features = torch.stack((curr_node_indicators, \n",
    "                                              self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "            return self.node_features.to(device), torch.tensor(reward).to(device), self.routing_done\n",
    "\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        action_mask = copy.deepcopy(self.adj_matrix[self.current_node])\n",
    "        action_mask = F.pad(action_mask, (0, self.n_nodes - action_mask.size(0)), \"constant\", 0).to(device)\n",
    "        return action_mask\n",
    "\n",
    "    def get_adj_matrix(self):\n",
    "        return copy.deepcopy(self.adj_matrix).to(device)\n",
    "    \n",
    "    def get_edge_index(self):\n",
    "        return copy.deepcopy(self.edge_index).to(device)\n",
    "        \n",
    "    def compute_reward(self, next_hop):\n",
    "        if self.routing_steps >= self.max_routing_steps:\n",
    "            print(\"Failed, \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return -2\n",
    "        elif next_hop == self.end_node:\n",
    "            print(\"Routing done, number of hops: \", self.routing_steps, \" minimum number of hops: \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return (self.min_n_hops / self.routing_steps) * 5 + 1\n",
    "        elif self.n_hop_matrix[self.current_node, self.end_node] > self.n_hop_matrix[next_hop, self.end_node]:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sim_done(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter <= self.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DummyEnv, self).__init__()\n",
    "        self.adj_matrix = torch.tensor([[1, 1, 0, 0, 1, 1, 0, 0],\n",
    "                                        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "                                        [0, 1, 1, 1, 0, 0, 1, 1],\n",
    "                                        [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "                                        [1, 1, 0, 0, 1, 0, 1, 0],\n",
    "                                        [1, 1, 0, 0, 0, 1, 0, 0],\n",
    "                                        [0, 0, 1, 0, 1, 0, 1, 0],\n",
    "                                        [0, 0, 1, 0, 0, 0, 0, 1]])\n",
    "        self.adj_matrix = self.adj_matrix - torch.eye(self.adj_matrix.size(0))\n",
    "        self.n_hop_matrix = bfs_distance(self.adj_matrix)\n",
    "        self.n_hop_matrix = self.n_hop_matrix - torch.diag(torch.diag(self.n_hop_matrix))\n",
    "        print(self.n_hop_matrix)\n",
    "        self.num_nodes = self.adj_matrix.size(0)\n",
    "        self.start_node = 0\n",
    "        self.end_node = 3\n",
    "        self.current_node = self.start_node\n",
    "        self.edge_index = dense_to_sparse(self.adj_matrix)[0]\n",
    "        self.routing_steps = 0\n",
    "        self.end_node_indicator = torch.zeros(self.num_nodes)\n",
    "        self.end_node_indicator[self.end_node] = 1\n",
    "        self.routing_done = False\n",
    "        self.max_routing_steps = 30\n",
    "        self.min_n_hops = 3\n",
    "        self.n_nodes = 8\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def get_edge_index(self):\n",
    "        return copy.deepcopy(self.edge_index).to(device)\n",
    "\n",
    "    def step(self):\n",
    "        self.routing_done = False\n",
    "        self.current_node = self.start_node\n",
    "        self.routing_steps = 0\n",
    "        self.current_node_indicator = torch.zeros(self.num_nodes)\n",
    "        self.current_node_indicator[self.current_node] = 1\n",
    "        self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "        self.node_features = torch.stack((self.current_node_indicator, self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "        return self.node_features.to(device)\n",
    "    \n",
    "    def act(self, neighbor_index):\n",
    "        self.routing_steps += 1\n",
    "        neighbors = torch.where(self.adj_matrix[self.current_node] == 1)[0]\n",
    "        valid_action_size = len(neighbors)\n",
    "        if valid_action_size <= neighbor_index:\n",
    "            if self.node_features.device != device:\n",
    "                self.node_features = self.node_features.to(device)\n",
    "            return self.node_features, torch.tensor(0).to(device), False\n",
    "        else:\n",
    "            next_hop = neighbors[neighbor_index]\n",
    "            reward = self.compute_reward(next_hop)\n",
    "            self.current_node = next_hop\n",
    "            curr_node_indicators = torch.zeros(self.n_nodes)\n",
    "            curr_node_indicators[self.current_node] = 1\n",
    "            self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "            self.node_features = torch.stack((curr_node_indicators, \n",
    "                                              self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "            return self.node_features.to(device), torch.tensor(reward).to(device), self.routing_done\n",
    "    \n",
    "    def compute_reward(self, next_hop):\n",
    "        if self.routing_steps >= self.max_routing_steps:\n",
    "            print(\"Failed, \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return -2\n",
    "        elif next_hop == self.end_node:\n",
    "            print(\"Routing done, number of hops: \", self.routing_steps, \" minimum number of hops: \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return (self.min_n_hops / self.routing_steps) * 5 + 1\n",
    "        elif self.n_hop_matrix[self.current_node, self.end_node] > self.n_hop_matrix[next_hop, self.end_node]:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        action_mask = copy.deepcopy(self.adj_matrix[self.current_node])\n",
    "        action_mask = F.pad(action_mask, (0, 4 - action_mask.size(0)), \"constant\", 0).to(device)\n",
    "        return action_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_nodes=57, hidden_dim=64, dropout=0.1, max_n_neighbors=15):\n",
    "        super(GDQN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.convs1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.convs2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * n_nodes, n_nodes)\n",
    "        self.fc2 = nn.Linear(n_nodes, max_n_neighbors)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.selu = nn.SELU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = self.selu(x)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = self.selu(x)\n",
    "        x = x.view(-1, self.n_nodes * self.hidden_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.selu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('data', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 1., 1., 2., 3.],\n",
      "        [1., 0., 1., 2., 1., 1., 2., 2.],\n",
      "        [2., 1., 0., 1., 2., 2., 1., 1.],\n",
      "        [3., 2., 1., 0., 3., 3., 2., 2.],\n",
      "        [1., 1., 2., 3., 0., 2., 1., 3.],\n",
      "        [1., 1., 2., 3., 2., 0., 3., 3.],\n",
      "        [2., 2., 1., 2., 1., 3., 0., 2.],\n",
      "        [3., 2., 1., 2., 3., 3., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00\n",
    "EPS_DECAY = 100\n",
    "TAU = 0.005\n",
    "LR = 0.0005\n",
    "\n",
    "\n",
    "n_nodes = 57\n",
    "# env = RoutingGym(sumoCmd, 1100, n_nodes)\n",
    "env = DummyEnv()\n",
    "\n",
    "policy_net = GDQN(n_nodes=8).to(device)\n",
    "target_net = GDQN(n_nodes=8).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(data, action_mask):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(data).max(1).indices.view(-1)\n",
    "    else:\n",
    "        valid_size = len(torch.where(action_mask == 1)[0])\n",
    "        return torch.randint(0, valid_size, (1,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = Batch.from_data_list([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    data_batch = Batch.from_data_list(batch.data)\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    reward_batch = torch.concat(batch.reward)\n",
    "  \n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(data_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing done, number of hops:  7  minimum number of hops:  3\n",
      "Episode: 1, Accumulated reward: 3.5428570806980133\n",
      "Routing done, number of hops:  11  minimum number of hops:  3\n",
      "Episode: 2, Accumulated reward: 2.5636362582445145\n",
      "Failed,  3\n",
      "Episode: 3, Accumulated reward: -1.8999999985098839\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 4, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 5, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 6, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 7, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 8, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 9, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 10, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 11, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 12, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 13, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 14, Accumulated reward: 4.950000002980232\n",
      "Failed,  3\n",
      "Episode: 15, Accumulated reward: -1.8999999985098839\n",
      "Routing done, number of hops:  4  minimum number of hops:  3\n",
      "Episode: 16, Accumulated reward: 4.950000002980232\n",
      "Routing done, number of hops:  10  minimum number of hops:  3\n",
      "Episode: 17, Accumulated reward: 3.0000000074505806\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 18, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 19, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 20, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 21, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 22, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 23, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 24, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 25, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 26, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 27, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  20  minimum number of hops:  3\n",
      "Episode: 28, Accumulated reward: 1.9500000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 29, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 30, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 31, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 32, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 33, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 34, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 35, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 36, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 37, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 38, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 39, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 40, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 41, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 42, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 43, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 44, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 45, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 46, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 47, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 48, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 49, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 50, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 51, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 52, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 53, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 54, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 55, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 56, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 57, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 58, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 59, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 60, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 61, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 62, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 63, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 64, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 65, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 66, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 67, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 68, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 69, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 70, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 71, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 72, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 73, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 74, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 75, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 76, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 77, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 78, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 79, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 80, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 81, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 82, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 83, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 84, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 85, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 86, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 87, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 88, Accumulated reward: 6.200000002980232\n",
      "Routing done, number of hops:  3  minimum number of hops:  3\n",
      "Episode: 89, Accumulated reward: 6.200000002980232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     40\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[345], line 11\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute a mask of non-final states and concatenate the batch elements\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# (a final state would've been the one after which simulation ended)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m                                       batch\u001b[38;5;241m.\u001b[39mnext_state)), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m---> 11\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_state\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(batch\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     14\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch\u001b[38;5;241m.\u001b[39maction)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/batch.py:97\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/collate.py:109\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# If parts of the data are already on GPU, make sure that auxiliary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# data like `batch` or `ptr` are also created on GPU:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/collate.py:169\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    167\u001b[0m slices \u001b[38;5;241m=\u001b[39m cumsum(sizes)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m increment:\n\u001b[0;32m--> 169\u001b[0m     incs \u001b[38;5;241m=\u001b[39m \u001b[43mget_incs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(incs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    171\u001b[0m         values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    172\u001b[0m             value \u001b[38;5;241m+\u001b[39m inc\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value, inc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, incs)\n\u001b[1;32m    174\u001b[0m         ]\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/collate.py:321\u001b[0m, in \u001b[0;36mget_incs\u001b[0;34m(key, values, data_list, stores)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    320\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 321\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    322\u001b[0m         data\u001b[38;5;241m.\u001b[39m__inc__(key, value, store)\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    324\u001b[0m     ]\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    326\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/collate.py:322\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_incs\u001b[39m(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    320\u001b[0m              stores: List[BaseStorage]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    321\u001b[0m     repeats \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 322\u001b[0m         \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inc__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value, data, store \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, data_list, stores)\n\u001b[1;32m    324\u001b[0m     ]\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repeats[\u001b[38;5;241m0\u001b[39m], Tensor):\n\u001b[1;32m    326\u001b[0m         repeats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(repeats, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/data.py:658\u001b[0m, in \u001b[0;36mData.__inc__\u001b[0;34m(self, key, value, *args, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(value\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/data.py:614\u001b[0m, in \u001b[0;36mData.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_nodes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/data.py:185\u001b[0m, in \u001b[0;36mBaseData.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of nodes in the graph.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    You will be given a warning that requests you to do so.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([v\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_stores])\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/data.py:185\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of nodes in the graph.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m.. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    You will be given a warning that requests you to do so.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_stores])\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/storage.py:423\u001b[0m, in \u001b[0;36mNodeStorage.num_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m N_KEYS:\n\u001b[1;32m    425\u001b[0m         cat_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent()\u001b[38;5;241m.\u001b[39m__cat_dim__(key, value, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch_geometric/data/storage.py:186\u001b[0m, in \u001b[0;36mBaseStorage.items\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitems\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ItemsView:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mItemsView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get its state\n",
    "n_epoch = 10\n",
    "for e in range(n_epoch):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    episode_num = 0\n",
    "    while not done:\n",
    "        # done = env.sim_done()\n",
    "        done = False\n",
    "        state = env.step()\n",
    "        episode_num += 1\n",
    "        edge_index = env.get_edge_index()\n",
    "        routing_done = False\n",
    "        accumulated_reward = 0\n",
    "        while not routing_done:\n",
    "            action_mask = env.get_action_mask()\n",
    "            data = Data(x=state, edge_index=edge_index)\n",
    "            action = select_action(data, action_mask)\n",
    "            node_features, reward, routing_done = env.act(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            accumulated_reward += reward.item()\n",
    "\n",
    "            routing_done = routing_done\n",
    "\n",
    "            if routing_done:\n",
    "                next_state = None\n",
    "                memory.push(data, action, None, reward)\n",
    "            else:\n",
    "                next_state = node_features\n",
    "                memory.push(data, action, Data(x=next_state, edge_index=edge_index), reward)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            optimize_model()\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "        print(f\"Episode: {episode_num}, Accumulated reward: {accumulated_reward}\")\n",
    "\n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
