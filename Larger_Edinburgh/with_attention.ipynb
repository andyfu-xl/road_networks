{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import copy\n",
    "from itertools import count\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "# sumoBinary = checkBinary('sumo-gui')\n",
    "sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervehicleConnectivity(threshold = None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for vehicle in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(vehicle)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        connectivity = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    return connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n"
     ]
    }
   ],
   "source": [
    "def randomTrips(dur=1000, density=12):\n",
    "    os.system(\"python $SUMO_HOME/tools/randomTrips.py -n config/osm.net.xml.gz -r config/osm.passenger.trips.xml -e \" + str(dur) + \" -l --insertion-density=\" + str(density))\n",
    "\n",
    "def shouldContinueSim():\n",
    "    numVehicles = traci.simulation.getMinExpectedNumber()\n",
    "    return True if numVehicles > 0 else False\n",
    "\n",
    "def restart(sumoCmd):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "def close():\n",
    "    traci.close()\n",
    "\n",
    "randomTrips(800, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_distance(adj_matrix):\n",
    "    n_hop_matrix = torch.ones_like(adj_matrix) * 100\n",
    "    for start_node in range(adj_matrix.size(0)):\n",
    "        visited = [0] * adj_matrix.size(0)\n",
    "        queue = deque([(start_node, 0)])\n",
    "        visited[start_node] = True\n",
    "        \n",
    "        while queue:\n",
    "            current_node, current_dist = queue.popleft()\n",
    "            \n",
    "            for neighbor, connected in enumerate(adj_matrix[current_node]):\n",
    "                if connected and not visited[neighbor]:\n",
    "                    queue.append((neighbor, current_dist + 1))\n",
    "                    visited[neighbor] = True\n",
    "                    n_hop_matrix[start_node, neighbor] = current_dist + 1\n",
    "    return n_hop_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoutingGym(gym.Env):\n",
    "    def __init__(self, sumoCmd, max_steps=1100, n_nodes=57, max_routing_steps=100):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.step_counter = 0\n",
    "        self.max_steps = max_steps\n",
    "        self.n_nodes = n_nodes\n",
    "        self.vehicle_ids = None\n",
    "        self.start_node = None\n",
    "        self.end_node = None\n",
    "        self.current_node = None\n",
    "        self.node_features = None\n",
    "        self.adj_matrix = None\n",
    "        self.edge_index = None\n",
    "        self.hop_thresh = None\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        self.min_n_hops = None\n",
    "        self.end_node_indicator = torch.zeros(n_nodes)\n",
    "        self.max_routing_steps = max_routing_steps\n",
    "        self.n_hop_matrix = None\n",
    "        self.neighbors_indicator = None\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        while self.step_counter < 400:\n",
    "            traci.simulationStep()\n",
    "            self.step_counter += 1\n",
    "\n",
    "    def step(self):\n",
    "        # traci.simulationStep()\n",
    "        self.end_node_indicator = torch.zeros(self.n_nodes)\n",
    "        self.routing_done = False\n",
    "        self.routing_steps = 0\n",
    "        self.step_counter += 1\n",
    "        self.vehicle_ids = traci.vehicle.getIDList()\n",
    "        self.adj_matrix = intervehicleConnectivity(800)\n",
    "        self.adj_matrix = self.adj_matrix - torch.eye(self.adj_matrix.size(0))\n",
    "        self.select_start_end_nodes()\n",
    "        self.current_node = self.start_node\n",
    "        self.adj_matrix = F.pad(self.adj_matrix, (0, self.n_nodes - self.adj_matrix.size(0), 0, self.n_nodes - self.adj_matrix.size(1)), \"constant\", 0)\n",
    "        self.n_hop_matrix = F.pad(self.n_hop_matrix, (0, self.n_nodes - self.n_hop_matrix.size(0), 0, self.n_nodes - self.n_hop_matrix.size(1)), \"constant\", 100)\n",
    "        # set diagonal to 0\n",
    "        self.n_hop_matrix = self.n_hop_matrix - torch.diag(torch.diag(self.n_hop_matrix))\n",
    "        self.edge_index, _ = dense_to_sparse(self.adj_matrix)\n",
    "        current_node_indicators = torch.zeros(self.n_nodes)\n",
    "        current_node_indicators[self.current_node] = 1\n",
    "        self.end_node_indicator[self.end_node] = 1\n",
    "        self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "        self.node_features = torch.stack((current_node_indicators, \n",
    "                                          self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "        \n",
    "        return self.node_features.to(device)\n",
    "\n",
    "    def select_start_end_nodes(self):\n",
    "        self.n_hop_matrix = bfs_distance(self.adj_matrix)\n",
    "        self.hop_thresh = min(self.n_hop_matrix.max(), 2)\n",
    "        starts, ends = torch.where(self.hop_thresh <= self.n_hop_matrix)\n",
    "        starts = starts.tolist()\n",
    "        ends = ends.tolist()\n",
    "        self.start_node, self.end_node = random.choice(list(zip(starts, ends)))\n",
    "        # minimal number of hops between start and end nodes\n",
    "        self.min_n_hops = self.n_hop_matrix[self.start_node, self.end_node]\n",
    "\n",
    "    def act(self, neighbor_index):\n",
    "        self.routing_steps += 1\n",
    "        neighbors = torch.where(self.adj_matrix[self.current_node] == 1)[0]\n",
    "        valid_action_size = len(neighbors)\n",
    "        if valid_action_size <= neighbor_index:\n",
    "            if self.node_features.device != device:\n",
    "                self.node_features = self.node_features.to(device)\n",
    "            return self.node_features, torch.tensor(0).to(device), False\n",
    "        else:\n",
    "            next_hop = neighbors[neighbor_index]\n",
    "            reward = self.compute_reward(next_hop)\n",
    "            self.current_node = next_hop\n",
    "            curr_node_indicators = torch.zeros(self.n_nodes)\n",
    "            curr_node_indicators[self.current_node] = 1\n",
    "            self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "            self.node_features = torch.stack((curr_node_indicators, \n",
    "                                              self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "            return self.node_features.to(device), torch.tensor(reward).to(device), self.routing_done\n",
    "    \n",
    "    def good_action(self):\n",
    "        current_distance_to_end = self.n_hop_matrix[self.current_node, self.end_node]\n",
    "        neighbors_closer_to_end = torch.where(self.n_hop_matrix[self.current_node] < current_distance_to_end)[0]\n",
    "        assert len(neighbors_closer_to_end) > 0\n",
    "        return torch.tensor([random.choice(neighbors_closer_to_end)], device=device)\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        action_mask = copy.deepcopy(self.adj_matrix[self.current_node])\n",
    "        action_mask = F.pad(action_mask, (0, self.n_nodes - action_mask.size(0)), \"constant\", 0).to(device)\n",
    "        return action_mask\n",
    "\n",
    "    def get_adj_matrix(self):\n",
    "        return copy.deepcopy(self.adj_matrix).to(device)\n",
    "    \n",
    "    def get_edge_index(self):\n",
    "        return copy.deepcopy(self.edge_index).to(device)\n",
    "        \n",
    "    def compute_reward(self, next_hop):\n",
    "        if self.routing_steps >= self.max_routing_steps:\n",
    "            print(\"Failed, \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return -2\n",
    "        elif next_hop == self.end_node:\n",
    "            print(\"Routing done, number of hops: \", self.routing_steps, \" minimum number of hops: \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return (self.min_n_hops / self.routing_steps) * 5 + 1\n",
    "        elif self.n_hop_matrix[self.current_node, self.end_node] > self.n_hop_matrix[next_hop, self.end_node]:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def sim_done(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter <= self.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_nodes=57, hidden_dim=32, max_n_neighbors=4):\n",
    "        super(GDQN, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.convs1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.convs2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim * n_nodes, n_nodes)\n",
    "        self.fc2 = nn.Linear(n_nodes, max_n_neighbors)\n",
    "        self.selu = nn.SELU()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # neighbor_mask = x[:, 3].reshape(-1, self.n_nodes)\n",
    "        # neighbor_num = torch.sum(neighbor_mask, dim=1)\n",
    "        # print(neighbor_num)\n",
    "        x = self.convs1(x, edge_index)\n",
    "        print(x.shape)\n",
    "        x = self.selu(x)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = self.selu(x)\n",
    "        x = x.reshape(-1, self.n_nodes * self.hidden_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.selu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        # Make sure that the d_model is divisible by the number of heads\n",
    "        assert self.head_dim * num_heads == d_model\n",
    "        \n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.query_linear(query)  # (batch_size, seq_len, d_model)\n",
    "        K = self.key_linear(key)      # (batch_size, seq_len, d_model)\n",
    "        V = self.value_linear(value)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Reshape and transpose for multi-head attention\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads and put through final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_linear(attn_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('data', 'action', 'next_state', 'reward', 'shuffle_indices'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DummyEnv, self).__init__()\n",
    "        self.adj_matrix = torch.tensor([[1, 1, 0, 0, 1, 1, 0, 0],\n",
    "                                        [1, 1, 1, 0, 1, 1, 0, 0],\n",
    "                                        [0, 1, 1, 1, 0, 0, 1, 1],\n",
    "                                        [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "                                        [1, 1, 0, 0, 1, 0, 1, 0],\n",
    "                                        [1, 1, 0, 0, 0, 1, 0, 0],\n",
    "                                        [0, 0, 1, 0, 1, 0, 1, 0],\n",
    "                                        [0, 0, 1, 0, 0, 0, 0, 1]])\n",
    "        self.adj_matrix = self.adj_matrix - torch.eye(self.adj_matrix.size(0))\n",
    "        self.n_hop_matrix = bfs_distance(self.adj_matrix)\n",
    "        self.n_hop_matrix = self.n_hop_matrix - torch.diag(torch.diag(self.n_hop_matrix))\n",
    "        print(self.n_hop_matrix)\n",
    "        self.num_nodes = self.adj_matrix.size(0)\n",
    "        self.start_node = 0\n",
    "        self.end_node = 3\n",
    "        self.current_node = self.start_node\n",
    "        self.edge_index = dense_to_sparse(self.adj_matrix)[0]\n",
    "        self.routing_steps = 0\n",
    "        self.end_node_indicator = torch.zeros(self.num_nodes)\n",
    "        self.end_node_indicator[self.end_node] = 1\n",
    "        self.routing_done = False\n",
    "        self.max_routing_steps = 30\n",
    "        self.min_n_hops = 3\n",
    "        self.n_nodes = 8\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def get_edge_index(self):\n",
    "        return copy.deepcopy(self.edge_index).to(device)\n",
    "\n",
    "    def step(self):\n",
    "        self.routing_done = False\n",
    "        self.select_start_end_nodes()\n",
    "        self.current_node = self.start_node\n",
    "        self.routing_steps = 0\n",
    "        self.current_node_indicator = torch.zeros(self.num_nodes)\n",
    "        self.current_node_indicator[self.current_node] = 1\n",
    "        self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "        self.node_features = torch.stack((self.current_node_indicator, self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "        return self.node_features.to(device)\n",
    "    \n",
    "    def select_start_end_nodes(self):\n",
    "        self.hop_thresh = min(self.n_hop_matrix.max(), 2)\n",
    "        starts, ends = torch.where(self.hop_thresh <= self.n_hop_matrix)\n",
    "        starts = starts.tolist()\n",
    "        ends = ends.tolist()\n",
    "        self.start_node, self.end_node = random.choice(list(zip(starts, ends)))\n",
    "        self.end_node_indicator = torch.zeros(self.num_nodes)\n",
    "        self.end_node_indicator[self.end_node] = 1\n",
    "        # minimal number of hops between start and end nodes\n",
    "        self.min_n_hops = self.n_hop_matrix[self.start_node, self.end_node]\n",
    "    \n",
    "    def act(self, neighbor_index):\n",
    "        self.routing_steps += 1\n",
    "        neighbors = torch.where(self.adj_matrix[self.current_node] == 1)[0]\n",
    "        valid_action_size = len(neighbors)\n",
    "        if valid_action_size <= neighbor_index:\n",
    "            if self.node_features.device != device:\n",
    "                self.node_features = self.node_features.to(device)\n",
    "            return self.node_features, torch.tensor(0).to(device), False\n",
    "        else:\n",
    "            next_hop = neighbors[neighbor_index]\n",
    "            reward = self.compute_reward(next_hop)\n",
    "            self.current_node = next_hop\n",
    "            curr_node_indicators = torch.zeros(self.n_nodes)\n",
    "            curr_node_indicators[self.current_node] = 1\n",
    "            self.neighbors_indicator = self.adj_matrix[self.current_node]\n",
    "            self.node_features = torch.stack((curr_node_indicators, \n",
    "                                              self.n_hop_matrix[self.current_node], self.end_node_indicator, self.neighbors_indicator)).T\n",
    "            return self.node_features.to(device), torch.tensor(reward).to(device), self.routing_done\n",
    "    \n",
    "    def compute_reward(self, next_hop):\n",
    "        if self.routing_steps >= self.max_routing_steps:\n",
    "            print(\"Failed, \", self.start_node, self.end_node)\n",
    "            self.routing_done = True\n",
    "            return -2\n",
    "        elif next_hop == self.end_node:\n",
    "            print(\"Routing done, number of hops: \", self.routing_steps, \" minimum number of hops: \", self.min_n_hops)\n",
    "            self.routing_done = True\n",
    "            return (self.min_n_hops / self.routing_steps) + 1\n",
    "        elif self.n_hop_matrix[self.current_node, self.end_node] > self.n_hop_matrix[next_hop, self.end_node]:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def good_action(self):\n",
    "        current_distance_to_end = self.n_hop_matrix[self.current_node, self.end_node]\n",
    "        # filter out the neighbors that are closer to the end node\n",
    "        closer_to_end_mask = self.n_hop_matrix[self.end_node] < current_distance_to_end\n",
    "        neighbors_closer_to_end_mask = closer_to_end_mask * self.neighbors_indicator\n",
    "        neighbors_closer_to_end = torch.where(neighbors_closer_to_end_mask == 1)[0]\n",
    "        neighbors = torch.where(self.adj_matrix[self.current_node] == 1)[0]\n",
    "        assert len(neighbors_closer_to_end) > 0\n",
    "        selected = torch.tensor([random.choice(neighbors_closer_to_end)])\n",
    "        # return the index of the selected neighbor\n",
    "        selected_neighbor_index = torch.where(neighbors == selected)[0]\n",
    "        return selected_neighbor_index.to(device)\n",
    "\n",
    "    def get_action_mask(self):\n",
    "        action_mask = copy.deepcopy(self.adj_matrix[self.current_node])\n",
    "        action_mask = F.pad(action_mask, (0, self.n_nodes - action_mask.size(0)), \"constant\", 0).to(device)\n",
    "        return action_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: Indices where value is 1 -> tensor([0, 2])\n",
      "Row 1: Indices where value is 1 -> tensor([1, 2])\n",
      "Row 2: Indices where value is 1 -> tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample 2D tensor with values 0 and 1\n",
    "tensor = torch.tensor([\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 1, 0, 0]\n",
    "], dtype=torch.int)\n",
    "\n",
    "# Find the indices where the value is 1 for each row\n",
    "def find_indices_where_1(tensor):\n",
    "    indices_list = []\n",
    "    for row in tensor:\n",
    "        indices = torch.nonzero(row == 1, as_tuple=True)[0]\n",
    "        indices_list.append(indices)\n",
    "    return indices_list\n",
    "\n",
    "# Use the function to find the indices\n",
    "indices_where_1 = find_indices_where_1(tensor)\n",
    "\n",
    "# Print the result\n",
    "for row_idx, indices in enumerate(indices_where_1):\n",
    "    print(f\"Row {row_idx}: Indices where value is 1 -> {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1],\n",
      "        [2],\n",
      "        [3]]), tensor([[4],\n",
      "        [5]]), tensor([[6]])]\n",
      "Padded Vectors:\n",
      "tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [0]],\n",
      "\n",
      "        [[6],\n",
      "         [0],\n",
      "         [0]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "# Create a list of vectors with different lengths\n",
    "vectors = [torch.tensor([[1], [2], [3]]), torch.tensor([[4], [5]]), torch.tensor([[6]])]\n",
    "print(vectors)\n",
    "# Use pad_sequence to pad them to the same length\n",
    "padded_vectors = rnn.pad_sequence(vectors, batch_first=True, padding_value=0)\n",
    "\n",
    "print(\"Padded Vectors:\")\n",
    "print(padded_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDQN_Attention(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_nodes=57, hidden_dim=32, max_n_neighbors=4):\n",
    "        super(GDQN_Attention, self).__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.convs1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.convs2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * max_n_neighbors, max_n_neighbors)\n",
    "        self.selu = nn.SELU()\n",
    "        self.cross_attn = CrossAttention(hidden_dim, 1)\n",
    "        self.max_n_neighbors = max_n_neighbors\n",
    "\n",
    "    def forward(self, data, shuffle_indices=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        batch_neighbor_mask = x[:, 3].reshape(-1, self.n_nodes)\n",
    "        x = self.convs1(x, edge_index)\n",
    "        x = self.selu(x)\n",
    "        x = self.convs2(x, edge_index)\n",
    "        x = self.selu(x)\n",
    "\n",
    "        keys = x.reshape(-1, self.n_nodes, self.hidden_dim)\n",
    "        values = x.reshape(-1, self.n_nodes, self.hidden_dim)\n",
    "        batch_size = keys.size(0)\n",
    "\n",
    "        querys = []\n",
    "        for i in range(batch_size):\n",
    "            neighbor_indices = torch.where(batch_neighbor_mask[i])\n",
    "            query = keys[i][neighbor_indices]\n",
    "            if query.size(0) < self.max_n_neighbors:\n",
    "                query = F.pad(query, (0, 0, 0, self.max_n_neighbors - query.size(0)), \"constant\", 0)\n",
    "            querys.append(query)\n",
    "        querys = torch.stack(querys)\n",
    "\n",
    "        if batch_size == 1:\n",
    "            shuffle_indices = shuffle_indices.unsqueeze(0)\n",
    "        if shuffle_indices is not None:\n",
    "            batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, shuffle_indices.size(1))\n",
    "            shuffled_query = querys[batch_indices, shuffle_indices]\n",
    "        else:\n",
    "            shuffled_query = querys\n",
    "        x = self.cross_attn(shuffled_query, keys, values)\n",
    "        x = x.reshape(-1, self.hidden_dim * self.max_n_neighbors)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 1., 1., 2., 3.],\n",
      "        [1., 0., 1., 2., 1., 1., 2., 2.],\n",
      "        [2., 1., 0., 1., 2., 2., 1., 1.],\n",
      "        [3., 2., 1., 0., 3., 3., 2., 2.],\n",
      "        [1., 1., 2., 3., 0., 2., 1., 3.],\n",
      "        [1., 1., 2., 3., 2., 0., 3., 3.],\n",
      "        [2., 2., 1., 2., 1., 3., 0., 2.],\n",
      "        [3., 2., 1., 2., 3., 3., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.00\n",
    "EPS_DECAY = 100\n",
    "TAU = 0.005\n",
    "LR = 0.001\n",
    "\n",
    "n_nodes = 57\n",
    "# env = RoutingGym(sumoCmd, 1100, n_nodes)\n",
    "env = DummyEnv()\n",
    "max_n_neighbors = 4\n",
    "\n",
    "policy_net = GDQN_Attention(n_nodes=8, max_n_neighbors=max_n_neighbors).to(device)\n",
    "target_net = GDQN_Attention(n_nodes=8, max_n_neighbors=max_n_neighbors).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(data, action_mask, shuffle_indices):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([policy_net(data, shuffle_indices).max(1).indices.item()], device=device), True\n",
    "    else:\n",
    "        valid_size = len(torch.where(action_mask == 1)[0])\n",
    "        return torch.randint(0, valid_size, (1,), device=device), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = Batch.from_data_list([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    data_batch = Batch.from_data_list(batch.data)\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    reward_batch = torch.concat(batch.reward)\n",
    "    shuffle_indices_batch = torch.stack(batch.shuffle_indices)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    # print(\"SHHS\")\n",
    "    # print(policy_net(data_batch, shuffle_indices_batch).shape)\n",
    "    state_action_values = policy_net(data_batch, shuffle_indices_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # we do not need to shuffle the indices for the target network\n",
    "        # firstly, we don't train the target network\n",
    "        # secondly, we only need the max value anyway\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 1, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 2, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "0 2\n",
      "Episode: 3, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 4, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 5, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5483/697872379.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return self.node_features.to(device), torch.tensor(reward).to(device), self.routing_done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 7, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 8, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 9, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 10, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 11, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 12, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 13, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 14, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 15, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 16, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 17, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 18, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 19, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 20, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 21, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 22, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(2.)\n",
      "0 2\n",
      "Episode: 23, Accumulated reward: 1.7666667476296425\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 24, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 25, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 26, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 27, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 28, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 29, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 30, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 31, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 32, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 33, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 34, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 35, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 36, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 37, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 38, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 39, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 40, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 41, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 42, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 43, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 44, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 45, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 46, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 47, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 48, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 49, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 50, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 51, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 52, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 53, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 54, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 55, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 56, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 57, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 58, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 59, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 60, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "0 2\n",
      "Episode: 61, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 62, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 63, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 64, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 65, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 66, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 67, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 68, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 69, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 70, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 71, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 72, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 73, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 74, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 75, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 76, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 77, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 78, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 79, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 80, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 81, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 82, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 83, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 84, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 85, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 86, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 87, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 88, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 89, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 90, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 91, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 92, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 93, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 94, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 95, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 96, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 97, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 98, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 99, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 100, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 101, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 102, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 103, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 104, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 105, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 106, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 107, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 108, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  3  minimum number of hops:  tensor(3.)\n",
      "Episode: 109, Accumulated reward: 2.2000000029802322\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 110, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n",
      "Episode: 111, Accumulated reward: 2.100000001490116\n",
      "Routing done, number of hops:  2  minimum number of hops:  tensor(2.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[1;32m     45\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[142], line 23\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m shuffle_indices_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(batch\u001b[38;5;241m.\u001b[39mshuffle_indices)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# columns of actions taken. These are the actions which would've been taken\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# for each batch state according to policy_net\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"SHHS\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(policy_net(data_batch, shuffle_indices_batch).shape)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m state_action_values \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_indices_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# on the \"older\" target_net; selecting their best reward with max(1).values\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# This is merged based on the mask, such that we'll have either the expected\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# state value or 0 in case the state was final.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[134], line 17\u001b[0m, in \u001b[0;36mGDQN_Attention.forward\u001b[0;34m(self, data, shuffle_indices)\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_neighbor_mask \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs1(x, edge_index)\n\u001b[0;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs2(x, edge_index)\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselu(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/modules/activation.py:618\u001b[0m, in \u001b[0;36mSELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sumo/lib/python3.10/site-packages/torch/nn/functional.py:1620\u001b[0m, in \u001b[0;36mselu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1618\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mselu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1620\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the environment and get its state\n",
    "n_epoch = 10\n",
    "for e in range(n_epoch):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    episode_num = 0\n",
    "    while not done:\n",
    "        # done = env.sim_done()\n",
    "        done = False\n",
    "        state = env.step()\n",
    "        episode_num += 1\n",
    "        edge_index = env.get_edge_index()\n",
    "        routing_done = False\n",
    "        accumulated_reward = 0\n",
    "        while not routing_done:\n",
    "            action_mask = env.get_action_mask()\n",
    "\n",
    "            data = Data(x=state, edge_index=edge_index)\n",
    "            shuffle_indices = torch.randperm(max_n_neighbors)\n",
    "            action, use_policy = select_action(data, action_mask, shuffle_indices)\n",
    "            \n",
    "            if use_policy:\n",
    "                node_features, reward, routing_done = env.act(shuffle_indices[action.item()])\n",
    "            else:\n",
    "                node_features, reward, routing_done = env.act(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            accumulated_reward += reward.item()\n",
    "\n",
    "            routing_done = routing_done\n",
    "\n",
    "            if routing_done:\n",
    "                memory.push(data, action, None, reward, shuffle_indices)\n",
    "            else:\n",
    "                next_state = node_features\n",
    "                memory.push(data, action, Data(x=next_state, edge_index=edge_index), reward, shuffle_indices)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            optimize_model()\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "        if env.start_node == 0 and env.end_node == 2:\n",
    "            print(env.start_node, env.end_node)\n",
    "        print(f\"Episode: {episode_num}, Accumulated reward: {accumulated_reward}\")\n",
    "\n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1,2,3])\n",
    "padded_t = F.pad(t, (0, 1), \"constant\", 0)\n",
    "padded_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Tensor:\n",
      "tensor([[ 0,  1,  2,  3,  0,  0],\n",
      "        [ 4,  5,  6,  7,  0,  0],\n",
      "        [ 8,  9, 10, 11,  0,  0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [2, 2, 2, 2, 2, 2]])\n",
      "\n",
      "Perturbed Tensors:\n",
      "tensor([[ 1,  0,  0,  0,  3,  2],\n",
      "        [ 5,  0,  4,  0,  7,  6],\n",
      "        [ 9,  0,  8,  0, 11, 10]])\n"
     ]
    }
   ],
   "source": [
    "# Create a sample tensor\n",
    "tensor = torch.arange(12).view(3, 4)\n",
    "\n",
    "# Specify the dimension to pad and pad size\n",
    "dim_to_pad = 1\n",
    "pad_size = 2\n",
    "\n",
    "padded_tensor = F.pad(tensor, (0, pad_size), \"constant\", 0)\n",
    "print(\"\\nPadded Tensor:\")\n",
    "print(padded_tensor)\n",
    "\n",
    "# Define multiple index vectors for perturbation, one for each data point in the batch\n",
    "index_vectors = torch.tensor([\n",
    "    [1, 4, 0, 5, 3, 2],\n",
    "    [2, 5, 1, 4, 0, 3],\n",
    "    [4, 0, 5, 1, 3, 2]\n",
    "])\n",
    "\n",
    "index_vectors = torch.tensor([[1, 4, 0, 5, 3, 2]])\n",
    "\n",
    "# Generate a batch index for gathering\n",
    "batch_size = padded_tensor.size(0)\n",
    "batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, index_vectors.size(1))\n",
    "print(batch_indices)\n",
    "# Perturb the padded tensor using the index vectors\n",
    "perturbed_tensors = padded_tensor[batch_indices, index_vectors]\n",
    "\n",
    "print(\"\\nPerturbed Tensors:\")\n",
    "print(perturbed_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
