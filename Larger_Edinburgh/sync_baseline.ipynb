{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import gym\n",
    "\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "sumoBinary = checkBinary('sumo-gui')\n",
    "# sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervehicleConnectivity(threshold = None):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for vehicle in traci.vehicle.getIDList():\n",
    "        x, y = traci.vehicle.getPosition(vehicle)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    xs = torch.tensor(xs, dtype=torch.float32).to(device).view(-1,1)\n",
    "    ys = torch.tensor(ys, dtype=torch.float32).to(device).view(-1,1)\n",
    "    intervehicle_distances = torch.sqrt((xs - xs.t())**2 + (ys - ys.t())**2)\n",
    "    if threshold is not None:\n",
    "        # make the distances 1 if less than the threshold, 0 otherwise\n",
    "        intervehicle_distances = torch.where(intervehicle_distances < threshold, torch.ones_like(intervehicle_distances), torch.zeros_like(intervehicle_distances))\n",
    "    return intervehicle_distances, xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomTrips(dur=1000, density=12):\n",
    "    os.system(\"python $SUMO_HOME/tools/randomTrips.py -n config/osm.net.xml.gz -r config/osm.passenger.trips.xml -e \" + str(dur) + \" -l --insertion-density=\" + str(density))\n",
    "\n",
    "def shouldContinueSim():\n",
    "    numVehicles = traci.simulation.getMinExpectedNumber()\n",
    "    return True if numVehicles > 0 else False\n",
    "\n",
    "def restart(sumoCmd):\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "def close():\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Knowledges:\n",
    "    def __init__(self):\n",
    "        self.knowledges = {}\n",
    "        self.delays = {}\n",
    "    \n",
    "    def add_observations(self, vehicles, observed_vehicles):\n",
    "        for vehicle, visibility in zip(vehicles, observed_vehicles):\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = []\n",
    "                self.delays[vehicle] = 0\n",
    "            self.knowledges[vehicle].append(int(visibility))\n",
    "            if visibility == 0:\n",
    "                self.delays[vehicle] += 1\n",
    "            else:\n",
    "                self.delays[vehicle] = 0\n",
    "    \n",
    "    def merge_knowledges(self, new_knowledges, new_delays):\n",
    "        prev_missing, prev_delay = self.evaluate_knowledge()\n",
    "        for vehicle, visibility in new_knowledges.items():\n",
    "            if vehicle not in self.knowledges:\n",
    "                self.knowledges[vehicle] = copy.deepcopy(visibility)\n",
    "                self.delays[vehicle] = new_delays[vehicle]\n",
    "            else:\n",
    "                for i in range(1, len(self.knowledges[vehicle])+1):\n",
    "                    if i > len(visibility):\n",
    "                        break\n",
    "                    self.knowledges[vehicle][-i] = visibility[-i] | self.knowledges[vehicle][-i]\n",
    "                self.delays[vehicle] = min(self.delays[vehicle], new_delays[vehicle])\n",
    "        new_missing, new_delay = self.evaluate_knowledge()\n",
    "        return copy.deepcopy(self.knowledges), copy.deepcopy(self.delays), prev_missing - new_missing, prev_delay - new_delay\n",
    "\n",
    "    def get_knowledges(self):\n",
    "        return copy.deepcopy(self.knowledges)\n",
    "    \n",
    "    def get_delays(self):\n",
    "        return copy.deepcopy(self.delays)\n",
    "    \n",
    "    def evaluate_knowledge(self):\n",
    "        observed = 0\n",
    "        delay = 0\n",
    "        num_vehicles = len(self.knowledges)\n",
    "        for vehicle, visibility in self.knowledges.items():\n",
    "            observed += sum(visibility)\n",
    "            delay += self.delays[vehicle]\n",
    "        return 1-(observed / num_vehicles), delay / num_vehicles\n",
    "\n",
    "class Beacon:\n",
    "    def __init__(self, trace_hidden):\n",
    "        self.trace_hidden = copy.deepcopy(trace_hidden)\n",
    "    \n",
    "    def update(self, trace_hidden):\n",
    "        self.trace_hidden = copy.deepcopy(trace_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'next_state', 'reward'))\n",
    "\n",
    "class GRU_RL(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRU_RL, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n",
      "Max action space:  15\n"
     ]
    }
   ],
   "source": [
    "# find the maximum action space\n",
    "randomTrips(1000, 1.5)\n",
    "restart(sumoCmd)\n",
    "max_action_space = 0\n",
    "total_actions = 0\n",
    "\n",
    "total_missing_gain = 0\n",
    "total_delay_gain = 0\n",
    "step = 0\n",
    "max_action_space = 0\n",
    "\n",
    "\n",
    "while shouldContinueSim():\n",
    "    step += 1\n",
    "    if step > 1100:\n",
    "        close()\n",
    "        break\n",
    "    traci.simulationStep()\n",
    "    ids = traci.vehicle.getIDList()\n",
    "    connectivity, xs, ys = intervehicleConnectivity(800)\n",
    "    # minus the diagonal\n",
    "    action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "    for i, vehicle in enumerate(ids):\n",
    "        # get non-zero indices except the diagonal\n",
    "        non_zero_indices = np.where(action_spaces[i] == 1)[0]\n",
    "        if len(non_zero_indices) > max_action_space:\n",
    "            max_action_space = len(non_zero_indices)\n",
    "print(\"Max action space: \", max_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumoGym(gym.Env):\n",
    "    def __init__(self, sumoCmd, max_action_space, max_steps=1100):\n",
    "        self.sumoCmd = sumoCmd\n",
    "        self.max_action_space = max_action_space\n",
    "        self.max_steps = max_steps\n",
    "        self.step_counter = 0\n",
    "        self.vehicle_knowledges = {}\n",
    "        self.vehicle_ids = None\n",
    "        \n",
    "\n",
    "    def act(self, vehicle, selected_index):\n",
    "        selected_index = random.choice(non_zero_indices)\n",
    "        receiver = ids[selected_index]\n",
    "        _, _, missing_gain, delay_gain = self.vehicle_knowledges[receiver].merge_knowledges(self.vehicle_knowledges[vehicle].get_knowledges(), self.vehicle_knowledges[vehicle].get_delays())\n",
    "        reward = self.getRewards(missing_gain, delay_gain)\n",
    "        return reward\n",
    "    \n",
    "    def step(self):\n",
    "        if self.getDoneState():\n",
    "            traci.close()\n",
    "            return True\n",
    "        else:\n",
    "            traci.simulationStep()\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            connectivity, xs, ys = intervehicleConnectivity(800)\n",
    "            # minus the diagonal\n",
    "            action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "            \n",
    "            for i, vehicle in enumerate(ids):\n",
    "                if vehicle not in self.vehicle_knowledges:\n",
    "                    self.vehicle_knowledges[vehicle] = Knowledges()\n",
    "                vehicle_knowledges[vehicle].add_observations(ids, connectivity[i])\n",
    "            return False\n",
    "        \n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        self.show_gui = True\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            traci.close()\n",
    "        except:\n",
    "            pass\n",
    "        traci.start(sumoCmd)\n",
    "\n",
    "        while not self.getCurrentStates():\n",
    "            traci.simulationStep()\n",
    "            self.step_counter += 1\n",
    "            self.vehicle_ids = traci.vehicle.getIDList()\n",
    "            if self.vehicle_ids:\n",
    "                break\n",
    "\n",
    "        return self.getCurrentStates()\n",
    "\n",
    "\n",
    "    def getCurrentStates(self):\n",
    "        \"\"\"\n",
    "        function: Get all the states of vehicles, observation space.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for vehicle in self.vehicle_ids:\n",
    "            state = []\n",
    "            for observed_vehicle in self.vehicle_ids:\n",
    "                if vehicle == observed_vehicle:\n",
    "                    state.append(0)\n",
    "                else:\n",
    "                    state.append(1)\n",
    "            states.append(state)\n",
    "        return states\n",
    "\n",
    "\n",
    "    def getRewards(self, missing_reduction, delay_reduction, missing_coefficient=2, delay_coefficient=3):\n",
    "        return missing_reduction * missing_coefficient + delay_reduction * delay_coefficient\n",
    "        \n",
    "\n",
    "    def getDoneState(self):\n",
    "        \"\"\"\n",
    "        function: get the done state of simulation.\n",
    "        \"\"\"\n",
    "        return not (shouldContinueSim() and self.step_counter <= self.max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = max_action_space\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success.\n",
      "Max action space:  15\n"
     ]
    }
   ],
   "source": [
    "# find the maximum action space\n",
    "restart(800, 1.5)\n",
    "max_action_space = 0\n",
    "vehicle_knowledges = {}\n",
    "total_actions = 0\n",
    "\n",
    "total_missing_gain = 0\n",
    "total_delay_gain = 0\n",
    "# total_large_delay = 0\n",
    "step = 0\n",
    "max_action_space = 0\n",
    "\n",
    "\n",
    "while shouldContinueSim():\n",
    "    step += 1\n",
    "    if step > 1100:\n",
    "        close()\n",
    "        break\n",
    "    traci.simulationStep()\n",
    "    ids = traci.vehicle.getIDList()\n",
    "    connectivity, xs, ys = intervehicleConnectivity(800)\n",
    "    # minus the diagonal\n",
    "    action_spaces = connectivity.to(\"cpu\") - torch.eye(connectivity.size(0))\n",
    "    \n",
    "    for i, vehicle in enumerate(ids):\n",
    "        if vehicle not in vehicle_knowledges:\n",
    "            vehicle_knowledges[vehicle] = Knowledges()\n",
    "        vehicle_knowledges[vehicle].add_observations(ids, connectivity[i])\n",
    "    for i, vehicle in enumerate(ids):\n",
    "        # get non-zero indices except the diagonal\n",
    "        non_zero_indices = np.where(action_spaces[i] == 1)[0]\n",
    "        # for index in non_zero_indices:\n",
    "        #     receiver = ids[index]\n",
    "        #     assert vehicle in vehicle_knowledges and receiver in vehicle_knowledges\n",
    "        #     vehicle_knowledges[receiver].merge_knowledges(vehicle_knowledges[vehicle].get_knowledges(), vehicle_knowledges[vehicle].get_delays())\n",
    "        # if len(non_zero_indices) > 0:\n",
    "        #     selected_index = random.choice(non_zero_indices)\n",
    "        #     receiver = ids[selected_index]\n",
    "        #     assert vehicle in vehicle_knowledges and receiver in vehicle_knowledges\n",
    "        #     _, _, missing_gain, delay_gain = vehicle_knowledges[receiver].merge_knowledges(vehicle_knowledges[vehicle].get_knowledges(), vehicle_knowledges[vehicle].get_delays())\n",
    "        #     total_missing_gain += missing_gain\n",
    "        #     total_delay_gain += delay_gain\n",
    "        #     total_actions += 1\n",
    "        if len(non_zero_indices) > max_action_space:\n",
    "            max_action_space = len(non_zero_indices)\n",
    "print(\"Max action space: \", max_action_space)\n",
    "#     for i, vehicle in enumerate(ids):\n",
    "#         total_states += 1\n",
    "#         missing, delay, large_delay = vehicle_knowledges[vehicle].evaluate_knowledge()\n",
    "#         total_missing += missing\n",
    "#         total_delay += delay\n",
    "#         total_large_delay += large_delay\n",
    "# print(\"Average missing: \", total_missing / total_states)\n",
    "# print(\"Average delay: \", total_delay / total_states)\n",
    "# print(\"Average large delay: \", total_large_delay / total_states)\n",
    "\n",
    "# print(\"Average missing gain: \", total_missing_gain / total_actions)\n",
    "# print(\"Average delay gain: \", total_delay_gain / total_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average missing:  0.035199839455880545\n",
      "Average delay:  1.4695487863602796\n",
      "Max delay:  0.5606060606060606\n"
     ]
    }
   ],
   "source": [
    "num_vehicles = len(vehicle_knowledges)\n",
    "total_missing = 0\n",
    "total_delay = 0\n",
    "step = 0\n",
    "max_delay = 0 \n",
    "for vehicle, knowledge in vehicle_knowledges.items():\n",
    "    m, d, l = knowledge.evaluate_knowledge() \n",
    "    total_missing += m\n",
    "    total_delay += d\n",
    "    max_delay = max(max_delay, l)\n",
    "print(\"Average missing: \", total_missing/num_vehicles)\n",
    "print(\"Average delay: \", total_delay/num_vehicles)\n",
    "print(\"Max delay: \", max_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
