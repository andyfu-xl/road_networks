{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMO_HOME found\n",
      "SUMO_HOME found\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import sumolib\n",
    "import traci\n",
    "from sumolib import checkBinary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from utils import *\n",
    "\n",
    "\n",
    "if 'SUMO_HOME' in os.environ:\n",
    "    print('SUMO_HOME found')\n",
    "    sys.path.append(os.path.join(os.environ['SUMO_HOME'], 'tools'))\n",
    "\n",
    "sumoBinary = checkBinary('sumo-gui')\n",
    "# sumoBinary = checkBinary('sumo')\n",
    "roadNetwork = \"./config/osm.sumocfg\"\n",
    "sumoCmd = [sumoBinary, \"-c\", roadNetwork, \"--start\", \"--quit-on-end\"]\n",
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \" + str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "***Starting server on port 40021 ***\n",
      "Loading net-file from './config/osm.net.xml.gz' ... done (72ms).\n",
      "Loading additional-files from './config/osm.poly.xml.gz' ... done (30ms).\n",
      "Loading done.\n",
      "Simulation version 1.20.0 started with time: 0.00.\n",
      "Simulation ended at time: 10241.00\n",
      "Reason: TraCI requested termination.\n",
      "Performance: \n",
      " Duration: 5.44s\n",
      " TraCI-Duration: 3.34s\n",
      " Real time factor: 1881.5\n",
      " UPS: 104088.370384\n",
      "Vehicles: \n",
      " Inserted: 3218\n",
      " Running: 0\n",
      " Waiting: 0\n",
      "Statistics (avg of 3218):\n",
      " RouteLength: 3256.87\n",
      " Speed: 18.58\n",
      " Duration: 176.06\n",
      " WaitingTime: 0.77\n",
      " TimeLoss: 35.48\n",
      " DepartDelay: 0.63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "planned_path = get_planned_path()\n",
    "checkpoints = list(planned_path.values())\n",
    "checkpoints = torch.tensor(checkpoints).float() / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_nearest(prediction, planned_path):\n",
    "    with torch.no_grad():\n",
    "        starts = planned_path[:, :-1, :]\n",
    "        to = planned_path[:, 1:, :]\n",
    "\n",
    "        prediction = prediction.unsqueeze(1).repeat(1, starts.shape[1], 1)\n",
    "        ap = prediction - starts\n",
    "        ab = to - starts\n",
    "        numerator = torch.einsum('ijk,ijk->ij', ap, ab)\n",
    "        denominator = torch.einsum('ijk,ijk->ij', ab, ab)\n",
    "        t = numerator / denominator\n",
    "        t = torch.nan_to_num(t, nan=0.0)\n",
    "        t = torch.clamp(t, 0, 1)\n",
    "        projections = starts + t.unsqueeze(2) * ab\n",
    "        diff = projections - prediction\n",
    "        distances = torch.norm(diff, dim=2)\n",
    "        min_indices = torch.argmin(distances, dim=1)\n",
    "        projections = projections[range(projections.shape[0]), min_indices]\n",
    "        return projections, min_indices\n",
    "\n",
    "\n",
    "def project_to_nearest_with_checkpoints(prediction, planned_path):\n",
    "    with torch.no_grad():\n",
    "        starts = planned_path[:, :-1, :]\n",
    "        to = planned_path[:, 1:, :]\n",
    "\n",
    "        prediction = prediction.unsqueeze(1).repeat(1, starts.shape[1], 1)\n",
    "        ap = prediction - starts\n",
    "        ab = to - starts\n",
    "        numerator = torch.einsum('ijk,ijk->ij', ap, ab)\n",
    "        denominator = torch.einsum('ijk,ijk->ij', ab, ab)\n",
    "        t = numerator / denominator\n",
    "        t = torch.nan_to_num(t, nan=0.0)\n",
    "        t = torch.clamp(t, 0, 1)\n",
    "        projections = starts + t.unsqueeze(2) * ab\n",
    "        diff = projections - prediction\n",
    "        distances = torch.norm(diff, dim=2)\n",
    "        min_indices = torch.argmin(distances, dim=1)\n",
    "        projections = projections[range(projections.shape[0]), min_indices]\n",
    "        next_checkpoint = to[range(to.shape[0]), min_indices]\n",
    "        pad_to = F.pad(to, (0, 0, 1, 0), value=0)\n",
    "        next_next_checkpoint = pad_to[range(to.shape[0]), min_indices + 1]\n",
    "        projections_with_checkpoints = torch.cat((projections, next_checkpoint, next_next_checkpoint), dim=1)\n",
    "        return projections_with_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame loading\n",
    "df = pd.read_csv('zhucheng_trajectories.csv')\n",
    "num_cols = df.shape[1]\n",
    "position_indices = [i for i in range(num_cols) if i % 4 == 1 or i % 4 == 2]\n",
    "position_df = df.iloc[:, position_indices]\n",
    "position_array = position_df.to_numpy()\n",
    "sequence_length = len(position_indices) // 2\n",
    "tensor_list = []\n",
    "\n",
    "for row in position_array:\n",
    "    reshaped_tensor = torch.tensor(row.reshape(sequence_length, 2))\n",
    "    tensor_list.append(reshaped_tensor)\n",
    "\n",
    "all_trajectories_tensor = torch.stack(tensor_list).float() / 10\n",
    "\n",
    "next_checkpoint = torch.zeros_like(all_trajectories_tensor)\n",
    "next_next_checkpoint = torch.zeros_like(all_trajectories_tensor)\n",
    "checkpoints_pad_1 = F.pad(checkpoints, (0, 0, 1, 0))\n",
    "for i in range(all_trajectories_tensor.shape[1]):\n",
    "    _, min_indices = project_to_nearest(all_trajectories_tensor[:, i], checkpoints)\n",
    "    next_checkpoint[:, i] = checkpoints_pad_1[range(checkpoints.shape[0]), min_indices+1]\n",
    "    next_next_checkpoint[:, i] = checkpoints_pad_1[range(checkpoints.shape[0]), min_indices+2]\n",
    "\n",
    "all_trajectories_tensor = torch.cat((all_trajectories_tensor, next_checkpoint, next_next_checkpoint), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break trajectories into 1000 by 1000 cells\n",
    "trajectory_list = []\n",
    "checkpoints_list = []\n",
    "for i in range(all_trajectories_tensor.shape[0]):\n",
    "    trajectory = all_trajectories_tensor[i]\n",
    "    path = checkpoints[i]\n",
    "    modif = ((trajectory[0, :2] // 200) * 200 - 20).repeat(1, 3)\n",
    "    trajectory_to_add = []\n",
    "    next_trajectory_to_add = []\n",
    "    for j in range(trajectory.shape[0]):\n",
    "        if trajectory[j:, :2].max() == 0:\n",
    "            break\n",
    "        trajectory_j = (trajectory[j] - modif).squeeze()\n",
    "        # if the vehicle move out of the current cell\n",
    "        if trajectory_j[0] < 0 or trajectory_j[1] < 0 or trajectory_j[0] > 240 or trajectory_j[1] > 240:\n",
    "            # we first add all the points in the current cell\n",
    "            trajectory_list.append(torch.stack(trajectory_to_add))\n",
    "            # then we add checkpoints\n",
    "            checkpoints_list.append(path-modif.squeeze()[:2])\n",
    "\n",
    "            # we reset the to add list\n",
    "            trajectory_to_add = []\n",
    "            modif = ((trajectory[j, :2] // 200) * 200 - 20).repeat(1, 3)\n",
    "            # reset the to_add list, adding a part of the history to the new trajectory\n",
    "            if j != trajectory.shape[0] - 1:\n",
    "                rewind = 0\n",
    "                while True:\n",
    "                    trajectory_k = (trajectory[j - rewind] - modif).squeeze()\n",
    "                    if trajectory_k[0] < 0 or trajectory_k[0] > 240 or trajectory_k[1] < 0 or trajectory_k[1] > 240:\n",
    "                        rewind += 1\n",
    "                    else:\n",
    "                        break\n",
    "                trajectory_to_add = trajectory_to_add[:-rewind]\n",
    "\n",
    "        else:\n",
    "            trajectory_to_add.append(trajectory_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = torch.stack(checkpoints_list)\n",
    "def pad_tensors(tensors):\n",
    "    max_length = max([t.shape[0] for t in tensors])\n",
    "    padded_tensors = []\n",
    "    for tensor in tensors:\n",
    "        padded_tensor = F.pad(tensor, (0, 0, 0, max_length - tensor.shape[0]))\n",
    "        padded_tensors.append(padded_tensor)\n",
    "    return torch.stack(padded_tensors)\n",
    "\n",
    "all_trajectories_tensor = pad_tensors(trajectory_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(x, y, z, p):\n",
    "    return torch.tensor(np.repeat(np.random.rand(x * y) < p, z).reshape(x, y, z)).float()\n",
    "\n",
    "def generate_masks(tensors, min_mask_ratio=0.0, max_mask_ratio=0.1, missing_ratio=0.6, complete_traj_ratio=0.75):\n",
    "    initial_masks = missing(tensors.shape[0], tensors.shape[1], tensors.shape[2], missing_ratio)\n",
    "    masks = []\n",
    "    for initial_mask in initial_masks:\n",
    "        if np.random.rand() < complete_traj_ratio:\n",
    "            masks.append(torch.zeros_like(initial_mask).tolist())\n",
    "            continue\n",
    "        seq_length = initial_mask.shape[0]\n",
    "        mask_start = np.random.randint(int(seq_length * min_mask_ratio), int(seq_length * max_mask_ratio))\n",
    "        mask = torch.zeros_like(initial_mask)\n",
    "        mask[:, :mask_start] = 1\n",
    "        mask = initial_mask * mask\n",
    "        mask[0] = 0\n",
    "        mask[1] = 0\n",
    "        masks.append(mask.tolist())\n",
    "    return torch.tensor(masks)\n",
    "\n",
    "# split the data into training and validation sets\n",
    "# because the data is randomly generated, we don't need to shuffle it\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * all_trajectories_tensor.shape[0])\n",
    "train_trajectories_tensor = all_trajectories_tensor[:train_size]\n",
    "val_trajectories_tensor = all_trajectories_tensor[train_size:]\n",
    "\n",
    "train_checkpoints = checkpoints[:train_size]\n",
    "val_checkpoints = checkpoints[train_size:]\n",
    "\n",
    "train_mask = generate_masks(train_trajectories_tensor)\n",
    "\n",
    "class DatasetWithPlans(Dataset):\n",
    "    def __init__(self, tensor, input_mask, checkpoints):\n",
    "        self.tensor = tensor.float().to(device)\n",
    "        self.input_mask = input_mask.float().to(device)\n",
    "        self.checkpoints = checkpoints.float().to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx], self.input_mask[idx], self.checkpoints[idx]\n",
    "    \n",
    "train_dataset = DatasetWithPlans(train_trajectories_tensor, train_mask, train_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, int(hidden_size/2))\n",
    "        self.fc2 = nn.Linear(int(hidden_size/2), num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, hidden = self.gru(x, h0)  \n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out, hidden\n",
    "\n",
    "class CustomMSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSE, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # where target is 0\n",
    "        mask = (target != 0).float().to(device)\n",
    "        mse = torch.mean(((output - target) ** 2) * mask)\n",
    "        directional_diff = torch.mean(torch.abs(torch.atan2(output[:, 1], output[:, 0]) - torch.atan2(target[:, 1], target[:, 0])))\n",
    "        return mse + directional_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1409316.1951219512\n",
      "Epoch 2, Loss: 1052961.8125\n",
      "Epoch 3, Loss: 569158.0091463415\n",
      "Epoch 4, Loss: 296455.896722561\n",
      "Epoch 5, Loss: 192758.89862804877\n",
      "Epoch 6, Loss: 118871.65091463414\n",
      "Epoch 7, Loss: 50575.27138910061\n",
      "Epoch 8, Loss: 25212.74423589939\n",
      "Epoch 9, Loss: 19688.82881573933\n",
      "Epoch 10, Loss: 18803.98856707317\n",
      "Epoch 11, Loss: 18054.394793254574\n",
      "Epoch 12, Loss: 19101.316763528965\n",
      "Epoch 13, Loss: 18856.239329268294\n",
      "Epoch 14, Loss: 18209.787252286584\n",
      "Epoch 15, Loss: 17902.569121570123\n",
      "Epoch 16, Loss: 17933.246212842987\n",
      "Epoch 17, Loss: 17792.257550495426\n",
      "Epoch 18, Loss: 17615.274533155487\n",
      "Epoch 19, Loss: 17110.773556592987\n",
      "Epoch 20, Loss: 17009.604563643294\n",
      "Epoch 21, Loss: 17539.299185403965\n",
      "Epoch 22, Loss: 16929.437571455794\n",
      "Epoch 23, Loss: 16974.317906821645\n",
      "Epoch 24, Loss: 16763.82352801067\n",
      "Epoch 25, Loss: 16511.466558689026\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, epochs, optimizer, criterion):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, masks, planned_path in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            hidden = None\n",
    "            loss = 0\n",
    "            # Autoregressive prediction\n",
    "            # Start with the first input and predict each subsequent step\n",
    "            seq_len = inputs.size(1)\n",
    "            current_input = inputs[:, 0, :].unsqueeze(1)\n",
    "            for t in range(1, seq_len):\n",
    "                prediction, hidden = model(current_input, hidden)\n",
    "\n",
    "                previous_input = inputs[:, t-1, :2]\n",
    "                if epoch > 10:\n",
    "                    projection_with_checkpoints = project_to_nearest_with_checkpoints(prediction, planned_path)\n",
    "                    current_input = (projection_with_checkpoints * masks[:, t, :] + inputs[:, t, :] * (1-masks[:, t, :])).unsqueeze(1)\n",
    "                else:\n",
    "                    current_input = inputs[:, t, :].unsqueeze(1)\n",
    "                if t > 1:\n",
    "                    loss += criterion(prediction - previous_input, inputs[:, t, :2]-previous_input) #(current_input-previous_input).squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    " \n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}')\n",
    "        if epoch >= 20 and total_loss / len(dataloader) < 2000:\n",
    "            break\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = GRUModel(input_size=6, hidden_size=128, num_layers=2, num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CustomMSE()\n",
    "train_model(model, train_dataloader, 25, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def checkModel(model, inputs, masks, paths):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         hidden = None\n",
    "#         seq_len = inputs.size(1)\n",
    "#         current_input = inputs[:, 0, :].unsqueeze(1)\n",
    "#         for t in range(1, seq_len):\n",
    "#             prediction, hidden = model(current_input, hidden)\n",
    "#             projection_with_checkpoints = project_to_nearest_with_checkpoints(prediction, paths)\n",
    "#             current_input = (projection_with_checkpoints * masks[:, t, :] + inputs[:, t, :] * (1-masks[:, t, :])).unsqueeze(1)\n",
    "#             print(projection_with_checkpoints[0][:2]*10, inputs[:, t, :][0][:2]*10)\n",
    "\n",
    "# # get the first batch in dataloader\n",
    "# val_mask = generate_masks(val_trajectories_tensor, missing_ratio=0.9, complete_traj_ratio=0)\n",
    "# val_dataset = DatasetWithPlans(val_trajectories_tensor, val_mask, val_checkpoints)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "# inputs, masks, paths = next(iter(val_dataloader))\n",
    "\n",
    "# checkModel(model, inputs, masks, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 149.22529564823446\n",
      "Validation Loss: 92.50053886624013\n",
      "Validation Loss: 71.34419328146492\n",
      "Validation Loss: 57.415350985597804\n",
      "Validation Loss: 50.296513481695236\n",
      "Validation Loss: 32.994690308156265\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for inputs, masks, paths in dataloader:\n",
    "        hidden = None\n",
    "        loss = 0\n",
    "        seq_len = inputs.size(1)\n",
    "        current_input = inputs[:, 0, :].unsqueeze(1)\n",
    "        for t in range(1, seq_len):\n",
    "            new_steps = int(sum((inputs[:, t, :2].reshape(-1) != 0).float().to(device))) / 2\n",
    "            if new_steps == 0:\n",
    "                break\n",
    "            steps += new_steps\n",
    "            prediction, hidden = model(current_input, hidden)\n",
    "            projection_with_checkpoints = project_to_nearest_with_checkpoints(prediction, paths)\n",
    "            current_input = (projection_with_checkpoints * masks[:, t, :] + inputs[:, t, :] * (1-masks[:, t, :])).unsqueeze(1)\n",
    "            loss += torch.norm((projection_with_checkpoints[:, :2] - inputs[:, t, :2]) * 10, dim=1) * (inputs[:, t, :2] != 0)[:, 0]\n",
    "        total_loss += loss.sum().item()\n",
    "    print(f'Validation Loss: {total_loss / steps}')\n",
    "\n",
    "# load model\n",
    "# model = GRUModel(input_size=6, hidden_size=256, num_layers=2, num_classes=2).to(device)\n",
    "# model.load_state_dict(torch.load('gru_model.pth'))\n",
    "for i in [0.9, 0.8, 0.7, 0.6, 0.5, 0]:\n",
    "    val_mask = generate_masks(val_trajectories_tensor, missing_ratio=i, complete_traj_ratio=0)\n",
    "    val_dataset = DatasetWithPlans(val_trajectories_tensor, val_mask, val_checkpoints)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    evaluate_model(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
